{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lunathanael/chessnn/blob/main/zero_policy_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aEOVZHf_fRtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3b19c3-18d8-473f-a374-8a47d423902d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chess\n",
            "  Downloading chess-1.10.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: chess\n",
            "Successfully installed chess-1.10.0\n",
            "Collecting cairosvg\n",
            "  Downloading CairoSVG-2.7.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cairocffi (from cairosvg)\n",
            "  Downloading cairocffi-1.6.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cssselect2 (from cairosvg)\n",
            "  Downloading cssselect2-0.7.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from cairosvg) (0.7.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from cairosvg) (9.4.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from cairosvg) (1.2.1)\n",
            "Requirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from cairocffi->cairosvg) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from cssselect2->cairosvg) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.1.0->cairocffi->cairosvg) (2.21)\n",
            "Installing collected packages: cssselect2, cairocffi, cairosvg\n",
            "Successfully installed cairocffi-1.6.1 cairosvg-2.7.1 cssselect2-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install chess\n",
        "import chess\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from typing import List\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Add, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "from threading import Thread, Event\n",
        "\n",
        "!pip install cairosvg\n",
        "import chess.svg\n",
        "import IPython\n",
        "from IPython.display import SVG, display\n",
        "\n",
        "import cairosvg\n",
        "from PIL import Image\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(x, filters):\n",
        "    \"\"\"Create a residual block.\"\"\"\n",
        "    y = Conv2D(filters, kernel_size=3, padding='same')(x)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = ReLU()(y)\n",
        "    y = Conv2D(filters, kernel_size=3, padding='same')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Add()([y, x])\n",
        "    y = ReLU()(y)\n",
        "    return y\n",
        "\n",
        "def make_network():\n",
        "    # Input layer\n",
        "    input_layer = Input(shape=(8, 8, 20))  # 8x8 grid with 20 features per cell\n",
        "\n",
        "    # Body\n",
        "    x = Conv2D(256, kernel_size=3, padding='same', activation='relu')(input_layer)\n",
        "    x = BatchNormalization()(x)\n",
        "    # 19 residual blocks\n",
        "    for _ in range(19):\n",
        "        x = residual_block(x, 256)  # Each block has two convolutional layers with 256 filters, kernel size 3x3\n",
        "\n",
        "    # Policy Head\n",
        "    policy_head = Conv2D(256, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    policy_head = BatchNormalization()(policy_head)\n",
        "    policy_head = Conv2D(73, kernel_size=1)(policy_head)  # Output 73 policies\n",
        "\n",
        "    # Value Head\n",
        "    value_head = Conv2D(1, kernel_size=1, activation='relu')(x)\n",
        "    value_head = BatchNormalization()(value_head)\n",
        "    value_head = Flatten()(value_head)\n",
        "    value_head = Dense(256, activation='relu')(value_head)\n",
        "    value_head = Dense(1, activation='tanh')(value_head)  # Output single value\n",
        "\n",
        "    # Create the model\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=[value_head, policy_head])\n",
        "    return model\n",
        "\n",
        "\n",
        "def ConstantPolicyHead(shape):\n",
        "    # Custom layer for policy head\n",
        "    class CPHead(tf.keras.layers.Layer):\n",
        "        def call(self, inputs):\n",
        "            return tf.constant(1/73, shape=shape)\n",
        "\n",
        "    return CPHead()\n",
        "\n",
        "def ConstantValueHead():\n",
        "    # Custom layer for value head\n",
        "    class CVHead(tf.keras.layers.Layer):\n",
        "        def call(self, inputs):\n",
        "            return tf.constant(0.5, shape=(1,))\n",
        "\n",
        "    return CVHead()\n",
        "\n",
        "class make_uniform_network():\n",
        "  def __init__(self):\n",
        "    self._sval =  np.full((1,1,), 0.5)\n",
        "    self._spolicy = array2 = np.full((1, 8, 8, 73), 1/73)\n",
        "\n",
        "    self._val =  np.full((1,), 0.5)\n",
        "    self._policy = array2 = np.full((8, 8, 73), 1/73)\n",
        "\n",
        "\n",
        "  def predict(self, input, verbose=0):\n",
        "    repeat_times = input.shape[0]\n",
        "    if repeat_times != 1:\n",
        "      # Repeat _val and _policy\n",
        "      repeated_val = np.repeat(self._val, repeat_times, axis=0)\n",
        "      repeated_policy = np.repeat(self._policy, repeat_times, axis=0)\n",
        "\n",
        "      return repeated_val, repeated_policy\n",
        "    return self._sval, self._spolicy"
      ],
      "metadata": {
        "id": "ABgGpmx0fWGi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helpers"
      ],
      "metadata": {
        "id": "P3mUyAMlzxsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    ### Self-Play\n",
        "    self.num_actors = 5 # not enough ram <- 5000\n",
        "\n",
        "    self.num_sampling_moves = 30\n",
        "    self.max_moves = 512  # for chess and shogi, 722 for Go.\n",
        "\n",
        "    self.num_simulations = 31 #was 800, testing with 31\n",
        "\n",
        "    self.uniform_num_simulations = 0 # avg number of legal moves is 31\n",
        "    self.uniform_num_sampling_moves = 100000 # all are sampled from softmax\n",
        "    self.uniform_softmax_temperature = -1 # -1 will return random action\n",
        "\n",
        "    # Root prior exploration noise.\n",
        "    self.root_dirichlet_alpha = 0.3  # for chess, 0.03 for Go and 0.15 for shogi.\n",
        "    self.root_exploration_fraction = 0.25\n",
        "\n",
        "    # Softmax function\n",
        "    self.softmax_temperature = 10 # reduce later on when fine-tuning, choose moves that it believes are more likely to be successful\n",
        "\n",
        "    # UCB formula\n",
        "    self.pb_c_base = 19652\n",
        "    self.pb_c_init = 1.25\n",
        "\n",
        "    ### Training\n",
        "    self.training_steps = int(700e3) # 700,000 take too long?\n",
        "    self.checkpoint_interval = int(1e3)\n",
        "    self.window_size = int(1e6)\n",
        "    self.batch_size = 4096\n",
        "\n",
        "    self.weight_decay = 1e-4\n",
        "    self.momentum = 0.9\n",
        "    # Schedule for chess and shogi, Go starts at 2e-2 immediately.\n",
        "    self.learning_rate_schedule = {\n",
        "        0: 2e-1,\n",
        "        100e3: 2e-2,\n",
        "        300e3: 2e-3,\n",
        "        500e3: 2e-4\n",
        "    }"
      ],
      "metadata": {
        "id": "t32QDQxky6Yn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Node(object):\n",
        "\n",
        "  def __init__(self, prior: float):\n",
        "    self.visit_count = 0\n",
        "    self.to_play = -1\n",
        "    self.prior = prior\n",
        "    self.value_sum = 0\n",
        "    self.children = {}\n",
        "\n",
        "  def expanded(self):\n",
        "    return len(self.children) > 0\n",
        "\n",
        "  def value(self):\n",
        "    if self.visit_count == 0:\n",
        "      return 0\n",
        "    return self.value_sum / self.visit_count\n"
      ],
      "metadata": {
        "id": "Lvmt70d1y8HR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fen_to_repr(fen, repeats):\n",
        "    # Split the FEN string to get the relevant parts\n",
        "    parts = fen.split(' ')\n",
        "    board_fen, player, castling, _, halfmove, fullmove = parts[:6]\n",
        "\n",
        "    # 12 pieces, 4 cf, col, rep, half, total\n",
        "    board = np.zeros((8, 8, 12 + 4 + 1 + 1 + 1 + 1), dtype=float)\n",
        "\n",
        "    # Define piece order and mapping to layers\n",
        "    piece_map = {'p': 0, 'n': 1, 'b': 2, 'r': 3, 'q': 4, 'k': 5,\n",
        "                 'P': 6, 'N': 7, 'B': 8, 'R': 9, 'Q': 10, 'K': 11}\n",
        "\n",
        "    # Fill the board with pieces\n",
        "    row = 0\n",
        "    col = 0\n",
        "    for char in board_fen:\n",
        "        if char.isdigit():\n",
        "            col += int(char)\n",
        "        elif char == '/':\n",
        "            row += 1\n",
        "            col = 0\n",
        "        else:\n",
        "            board[row, col, piece_map[char]] = 1\n",
        "            col += 1\n",
        "\n",
        "    # Castling rights encoded in four binary planes\n",
        "    board[:, :, 12] = 1 if 'K' in castling else 0  # White kingside\n",
        "    board[:, :, 13] = 1 if 'Q' in castling else 0  # White queenside\n",
        "    board[:, :, 14] = 1 if 'k' in castling else 0  # Black kingside\n",
        "    board[:, :, 15] = 1 if 'q' in castling else 0  # Black queenside\n",
        "\n",
        "    # Player color (1 for black, 0 for white)\n",
        "    board[:, :, 16] = 1 if player == 'b' else 0\n",
        "\n",
        "    # Position repetitions\n",
        "    board[:, :, 17] = repeats\n",
        "\n",
        "    board[:, :, 18] = float(halfmove)\n",
        "\n",
        "    # Move number (as a real value)\n",
        "    board[:, :, 19] = float(fullmove)\n",
        "\n",
        "    return board"
      ],
      "metadata": {
        "id": "1LZ0finYdR6k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def square_to_coord(square):\n",
        "  #       r           c\n",
        "  return (square // 8, square % 8)\n",
        "\n",
        "knight_moves = [\n",
        "    (1, 2), (1, -2),\n",
        "    (2, 1), (2, -1),\n",
        "    (-1, 2), (-1, -2),\n",
        "    (-2, 1), (-2, -1)\n",
        "]\n",
        "\n",
        "promotion_pieces = \"nbr\"\n",
        "rows = \"abcdefgh\"\n",
        "\n",
        "queen_moves = [\n",
        "    (1, 1), (1, -1), (1, 0),\n",
        "    (-1, 1), (-1, -1), (-1, 0),\n",
        "    (0, 1), (0, -1)\n",
        "]\n",
        "\n",
        "def coord_to_uci(fr, fc, tr, tc):\n",
        "  return rows[fc] + str(fr + 1) + rows[tc] + str(tr + 1)\n",
        "\n",
        "def decode_action(action):\n",
        "\n",
        "  #piece = self.board.piece_at(move.from_square)\n",
        "  #fr, fc = square_to_coord(move.from_square)\n",
        "  #tr, tc = square_to_coord(move.to_square)\n",
        "  fr = action[0]\n",
        "  fc = action[1]\n",
        "\n",
        "  if action[2] < 9:\n",
        "    piece_idx = action[2] // 3\n",
        "    move_idx = action[2] % 3\n",
        "    if fr == 1: # black promo\n",
        "      tr = 0\n",
        "    else:\n",
        "      tr = 7\n",
        "    if move_idx == 0:\n",
        "      tc = fc\n",
        "    elif move_idx == 1:\n",
        "      tc = fc - 1\n",
        "    else:\n",
        "      tc = fc + 1\n",
        "    return coord_to_uci(fr, fc, tr, tc) + promotion_pieces[piece_idx]\n",
        "\n",
        "  if action[2] < 17:\n",
        "    tr = fr + knight_moves[action[2] - 9][0]\n",
        "    tc = fc + knight_moves[action[2] - 9][1]\n",
        "  else:\n",
        "    dist = (action[2] - 17) // 8 + 1\n",
        "    status = (action[2] - 17) % 8\n",
        "    tr = fr + (queen_moves[status][0] * dist)\n",
        "    tc = fc + (queen_moves[status][1] * dist)\n",
        "\n",
        "  return coord_to_uci(fr, fc, tr, tc)\n",
        "\n",
        "\n",
        "class Environment(object):\n",
        "\n",
        "  def __init__(self, env=None):\n",
        "    if env == None:\n",
        "      self.board = chess.Board(chess.STARTING_FEN)\n",
        "      self.board_history = []\n",
        "      self.repetitions = {}\n",
        "\n",
        "      self.update_history()\n",
        "    else:\n",
        "      self.board = chess.Board.copy(env.board)\n",
        "      self.board_history = env.board_history.copy()\n",
        "      self.repetitions = env.repetitions.copy()\n",
        "\n",
        "  def is_terminal(self):\n",
        "    return self.board.is_game_over()\n",
        "\n",
        "  def terminal_value(self, to_play: int) -> int: # THIS FUNCTION COULD BE VERY INCORRECT, ASSUMING CLASSIC MCTS\n",
        "    # outcome = self.board.outcome(claim_draw=True)\n",
        "    # if not outcome:\n",
        "    #   return 0 # ERROR, SHOULD NEVER BE CALLED LIKE SO.\n",
        "    winner = self.board.outcome().winner\n",
        "\n",
        "    if winner != None:      # -> white node  ->white node\n",
        "      if winner == to_play: # white_won(True) and black(1) to play or other way,\n",
        "        return 1\n",
        "      return 0\n",
        "\n",
        "    return 0.5\n",
        "\n",
        "  def generate_legal_moves(self):\n",
        "    return self.board.generate_legal_moves()\n",
        "\n",
        "  def encode_action(self, move): #12 piece types, 4 flags, 2 coords\n",
        "\n",
        "    piece = self.board.piece_at(move.from_square)\n",
        "    fr, fc = square_to_coord(move.from_square)\n",
        "    tr, tc = square_to_coord(move.to_square)\n",
        "\n",
        "    if move.promotion and move.promotion != 5: # 9 planes\n",
        "      # move.promotion -> 2 : N, 3 : B, 4 : R, 5 : Q\n",
        "      piece_idx = move.promotion - 2\n",
        "      if fc == tc:\n",
        "        move_idx = 0\n",
        "      elif tc < fc: # left from white perspective\n",
        "        move_idx = 1\n",
        "      else:\n",
        "        move_idx = 2\n",
        "      plane = piece_idx * 3 + move_idx\n",
        "\n",
        "    elif str(piece).lower() == 'n': # 8 planes\n",
        "      for idx in range(len(knight_moves)):\n",
        "        if fr + knight_moves[idx][0] == tr and fc + knight_moves[idx][1] == tc:\n",
        "          plane = idx + 9\n",
        "          break\n",
        "\n",
        "    else: # 7 * 8 planes\n",
        "      if fr < tr: # moving forward\n",
        "        row_status = 0\n",
        "      elif fr > tr: # back\n",
        "        row_status = 1\n",
        "      else: # horizontal movement\n",
        "        row_status = 2\n",
        "\n",
        "      if fc < tc: # moving right\n",
        "        col_status = 0\n",
        "      elif fc > tc: # left\n",
        "        col_status = 1\n",
        "      else: # vertical movement\n",
        "        col_status = 2\n",
        "\n",
        "      # row status and col status should never both be 2, means something stinky\n",
        "\n",
        "      dist = max(abs(fr - tr), abs(fc - tc)) - 1\n",
        "      plane = dist * 8 + (row_status * 3 + col_status) + 17\n",
        "\n",
        "    # if decode_action((fr, fc, plane)) != str(move):\n",
        "    #   print(\"found diff\")\n",
        "    #   print(\"action: \", fr, fc, plane)\n",
        "    #   print(\"decoded: \", decode_action((fr, fc, plane)))\n",
        "    #   print(\"Coords: \", fr, fc, tr, tc)\n",
        "    #   print(move)\n",
        "    #   print(str(move) == coord_to_uci(fr, fc, tr, tc))\n",
        "    #   raise Exception(\"found differing in uci\")\n",
        "\n",
        "    return (fr, fc, plane)\n",
        "\n",
        "  def apply(self, action):\n",
        "    uci = decode_action(action)\n",
        "    if action[0] == 1 and len(uci) == 4 and str(self.board.piece_at(action[0]*8+action[1])) == 'p':\n",
        "      uci += 'q'\n",
        "    if action[0] == 6 and len(uci) == 4 and str(self.board.piece_at(action[0]*8+action[1])) == 'P':\n",
        "      uci += 'q'\n",
        "    self.board.push_uci(uci)\n",
        "    self.update_history()\n",
        "    #print(self.board)\n",
        "\n",
        "  def generate_legal_actions(self):\n",
        "    moves = self.generate_legal_moves()\n",
        "    actions = [self.encode_action(move) for move in moves]\n",
        "    return actions\n",
        "\n",
        "  def update_history(self):\n",
        "    fen = self.board.fen()\n",
        "    s_fen = fen.split(' ')\n",
        "    cs_fen = ' '.join(s_fen[:-2])\n",
        "\n",
        "    if cs_fen in self.repetitions:\n",
        "      self.repetitions[cs_fen] += 1\n",
        "      repeats = self.repetitions[cs_fen]\n",
        "    else:\n",
        "      self.repetitions[cs_fen] = 1\n",
        "      repeats = 1\n",
        "\n",
        "    halfmove_clock = self.board.halfmove_clock\n",
        "    self.board_history.append((fen, repeats))\n",
        "\n",
        "  def make_image(self, state_index=-1):\n",
        "    fen, repeats= self.board_history[state_index]\n",
        "\n",
        "    repr = fen_to_repr(fen, repeats)\n",
        "    return repr"
      ],
      "metadata": {
        "id": "3FE-c9peKBP2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Game(object):\n",
        "\n",
        "  def __init__(self, history=None, environment=None):\n",
        "    self.history = history or []\n",
        "    self.child_visits = []\n",
        "    self.num_actions = 4672  # action space size for chess\n",
        "    self.environment = Environment(environment)\n",
        "\n",
        "  def terminal(self):\n",
        "    # Game specific termination rules.\n",
        "    return self.environment.is_terminal()\n",
        "\n",
        "  def terminal_value(self, to_play):\n",
        "    # Game specific value.\n",
        "    return self.environment.terminal_value(to_play)\n",
        "\n",
        "  def legal_actions(self):\n",
        "    # Game specific calculation of legal actions.\n",
        "    return self.environment.generate_legal_actions()\n",
        "\n",
        "  def clone(self):\n",
        "    return Game(list(self.history), self.environment)\n",
        "\n",
        "  def apply(self, action):\n",
        "    self.history.append(action)\n",
        "    self.environment.apply(action)\n",
        "\n",
        "  def store_search_statistics(self, root):\n",
        "    sum_visits = sum(child.visit_count for child in root.children.values())\n",
        "    self.child_visits.append([\n",
        "        root.children[a].visit_count / sum_visits if a in root.children else 0\n",
        "        for a in range(self.num_actions)\n",
        "    ])\n",
        "\n",
        "  def make_image(self, state_index: int):\n",
        "    # Game specific feature planes.\n",
        "    return self.environment.make_image(state_index)\n",
        "\n",
        "  def make_target(self, state_index: int):\n",
        "    return (self.terminal_value(state_index % 2),\n",
        "            self.child_visits[state_index])\n",
        "\n",
        "  def to_play(self): # 0: White, 1: Black\n",
        "    return len(self.history) % 2\n"
      ],
      "metadata": {
        "id": "rSCKGOM9zjwa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, config: Config):\n",
        "    self.window_size = config.window_size\n",
        "    self.batch_size = config.batch_size\n",
        "    self.buffer = []\n",
        "\n",
        "  def save_game(self, game):\n",
        "    if len(self.buffer) > self.window_size:\n",
        "      with open(f'games/{name}.rb', 'wb') as buffer_file:\n",
        "        pickle.dump(self, buffer_file)\n",
        "      self.buffer.pop(0)\n",
        "    self.buffer.append(game)\n",
        "\n",
        "  def sample_batch(self):\n",
        "    # Sample uniformly across positions.\n",
        "    print(\"Sample from: \", len(self.buffer))\n",
        "    move_sum = float(sum(len(g.history) for g in self.buffer))\n",
        "    games = np.random.choice(\n",
        "        self.buffer,\n",
        "        size=self.batch_size,\n",
        "        p=[len(g.history) / move_sum for g in self.buffer])\n",
        "    game_pos = [(g, np.random.randint(len(g.history))) for g in games]\n",
        "    return [(g.make_image(i), g.make_target(i)) for (g, i) in game_pos]\n",
        "\n",
        "  def size(self):\n",
        "    return len(self.buffer)"
      ],
      "metadata": {
        "id": "En06LbK6zoRk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(object):\n",
        "\n",
        "  def __init__(self, uniform_model: bool=False):\n",
        "    if uniform_model:\n",
        "      self.model = make_uniform_network()\n",
        "    else:\n",
        "      self.model = make_network()\n",
        "\n",
        "  def inference(self, image): # inference for SINGLE IMAGE\n",
        "      # Run the neural network model to get predictions\n",
        "      image = np.expand_dims(image, axis=0)\n",
        "      value, policy_logits = self.model.predict(image, verbose=0)\n",
        "\n",
        "      value = value[0] # The value output is a scalar representing the predicted game outcome\n",
        "      policy_logits = policy_logits[0]\n",
        "\n",
        "      return value, np.array(policy_logits)\n",
        "\n",
        "  def batch_inference(self, images):\n",
        "    # Run the neural network model to get predictions\n",
        "    values, policy_logits = self.model.predict(images, verbose=0)\n",
        "\n",
        "    return values, np.array(policy_logits)\n",
        "\n",
        "\n",
        "  def get_weights(self):\n",
        "    # Returns the weights of this network.\n",
        "    return self.model.get_weights()"
      ],
      "metadata": {
        "id": "HPr_d9UOzqBG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SharedStorage(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._networks = {}\n",
        "\n",
        "  def latest_network(self) -> Network:\n",
        "    if self._networks:\n",
        "      return self._networks[max(self._networks.keys())]\n",
        "    else:\n",
        "      return None  # policy -> uniform, value -> 0.5\n",
        "\n",
        "  def save_network(self, step: int, network: Network):\n",
        "    self._networks[step] = network\n"
      ],
      "metadata": {
        "id": "6UsTnTQKzreQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training is split into two independent parts: Network training and\n",
        "# self-play data generation.\n",
        "# These two parts only communicate by transferring the latest network checkpoint\n",
        "# from the training to the self-play, and the finished games from the self-play\n",
        "# to the training.\n",
        "def zero(config: Config):\n",
        "  storage = SharedStorage()\n",
        "  replay_buffer = ReplayBuffer(config)\n",
        "  run_selfplay(config, storage, replay_buffer)\n",
        "  threads = [Thread(target=run_selfplay, args=(config, storage, replay_buffer))\n",
        "            for _ in range(config.num_actors)]\n",
        "\n",
        "  for t in threads:\n",
        "    t.start()\n",
        "\n",
        "  print(\"Self-play data generation launched.\")\n",
        "\n",
        "  train_network(config, storage, replay_buffer)\n",
        "\n",
        "  return storage.latest_network()\n",
        "\n"
      ],
      "metadata": {
        "id": "tV9QkBmLBKT9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "####### Part 1: Self-Play ########\n",
        "\n",
        "\n",
        "# Each self-play job is independent of all others; it takes the latest network\n",
        "# snapshot, produces a game and makes it available to the training job by\n",
        "# writing it to a shared replay buffer.\n",
        "def run_selfplay(config: Config, storage: SharedStorage,\n",
        "                 replay_buffer: ReplayBuffer):\n",
        "  while True:\n",
        "    network = storage.latest_network()\n",
        "    game = play_game(config, network)\n",
        "    replay_buffer.save_game(game)\n",
        "\n",
        "\n",
        "# Each game is produced by starting at the initial board position, then\n",
        "# repeatedly executing a Monte Carlo Tree Search to generate moves until the end\n",
        "# of the game is reached.\n",
        "def play_game(config: Config, network: Network=None):\n",
        "  num_simulations = config.num_simulations\n",
        "  num_sampling_moves = config.num_sampling_moves\n",
        "  softmax_temperature = config.softmax_temperature\n",
        "  if not network:\n",
        "    network = Network(True)\n",
        "    num_simulations = config.uniform_num_simulations\n",
        "    num_sampling_moves = config.uniform_num_sampling_moves\n",
        "    softmax_temperature = config.uniform_softmax_temperature\n",
        "\n",
        "  game = Game()\n",
        "  while not game.terminal() and len(game.history) < config.max_moves:\n",
        "    action, root = run_mcts(config, game, network, num_simulations, num_sampling_moves, softmax_temperature)\n",
        "    game.apply(action)\n",
        "    game.store_search_statistics(root)\n",
        "  return game\n",
        "\n",
        "\n",
        "# Core Monte Carlo Tree Search algorithm.\n",
        "# To decide on an action, we run N simulations, always starting at the root of\n",
        "# the search tree and traversing the tree according to the UCB formula until we\n",
        "# reach a leaf node.\n",
        "def run_mcts(config: Config, game: Game, network: Network, num_simulations: int=800, num_sampling_moves=30, softmax_temperature=10):\n",
        "  root = Node(0)\n",
        "  evaluate(root, game, network)\n",
        "  add_exploration_noise(config, root)\n",
        "\n",
        "  for _ in range(num_simulations):\n",
        "    node = root\n",
        "    scratch_game = game.clone()\n",
        "    search_path = [node]\n",
        "\n",
        "    while node.expanded():\n",
        "      action, node = select_child(config, node)\n",
        "      scratch_game.apply(action)\n",
        "      search_path.append(node)\n",
        "\n",
        "    value = evaluate(node, scratch_game, network)\n",
        "    backpropagate(search_path, value, scratch_game.to_play())\n",
        "\n",
        "  return select_action(config, game, root, num_sampling_moves, softmax_temperature), root\n",
        "\n",
        "\n",
        "\n",
        "def softmax_sample(visit_counts, temperature=10.0):\n",
        "  counts, actions = zip(*visit_counts)\n",
        "\n",
        "  if temperature == -1: # pure random\n",
        "    rnd_idx = np.random.choice(len(actions))\n",
        "    return None, actions[rnd_idx]\n",
        "\n",
        "  # Apply softmax with temperature\n",
        "  counts = np.array(counts)\n",
        "  counts = counts / temperature  # Apply temperature scaling\n",
        "  softmax_probs = np.exp(counts) / sum(np.exp(counts))\n",
        "  # Sample an action based on the softmax probabilities\n",
        "  rnd_idx = np.random.choice(len(actions), p=softmax_probs)\n",
        "  action = actions[rnd_idx]\n",
        "  return softmax_probs, action\n",
        "\n",
        "\n",
        "def select_action(config: Config, game: Game, root: Node, num_sampling_moves=30, softmax_temperature=10):\n",
        "  visit_counts = [(child.visit_count, action)\n",
        "                  for action, child in root.children.items()]\n",
        "  if len(game.history) < num_sampling_moves:\n",
        "    _, action = softmax_sample(visit_counts, softmax_temperature)\n",
        "  else:\n",
        "    _, action = max(visit_counts)\n",
        "  return action\n",
        "\n",
        "\n",
        "# Select the child with the highest UCB score.\n",
        "def select_child(config: Config, node: Node):\n",
        "  _, action, child = max((ucb_score(config, node, child), action, child)\n",
        "                         for action, child in node.children.items())\n",
        "  return action, child\n",
        "\n",
        "\n",
        "# The score for a node is based on its value, plus an exploration bonus based on\n",
        "# the prior.\n",
        "def ucb_score(config: Config, parent: Node, child: Node):\n",
        "  pb_c = math.log((parent.visit_count + config.pb_c_base + 1) /\n",
        "                  config.pb_c_base) + config.pb_c_init\n",
        "  pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "  prior_score = pb_c * child.prior\n",
        "  value_score = child.value()\n",
        "  return prior_score + value_score\n",
        "\n",
        "\n",
        "# We use the neural network to obtain a value and policy prediction.\n",
        "def evaluate(node: Node, game: Game, network: Network):\n",
        "  value, policy_logits = network.inference(game.make_image(-1))\n",
        "\n",
        "  # Expand the node.\n",
        "  node.to_play = game.to_play()\n",
        "\n",
        "  policy = {a: math.exp(policy_logits[a]) for a in game.legal_actions()}\n",
        "  policy_sum = sum(policy.values())\n",
        "  for action, p in policy.items():\n",
        "    node.children[action] = Node(p / policy_sum)\n",
        "  return value\n",
        "\n",
        "\n",
        "# At the end of a simulation, we propagate the evaluation all the way up the\n",
        "# tree to the root.\n",
        "def backpropagate(search_path: List[Node], value: float, to_play):\n",
        "  for node in search_path:\n",
        "    node.value_sum += value if node.to_play == to_play else (1 - value)\n",
        "    node.visit_count += 1\n",
        "\n",
        "\n",
        "# At the start of each search, we add dirichlet noise to the prior of the root\n",
        "# to encourage the search to explore new actions.\n",
        "def add_exploration_noise(config: Config, node: Node):\n",
        "  actions = node.children.keys()\n",
        "  noise = np.random.gamma(config.root_dirichlet_alpha, 1, len(actions))\n",
        "  frac = config.root_exploration_fraction\n",
        "  for a, n in zip(actions, noise):\n",
        "    node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "######### End Self-Play ##########\n",
        "##################################\n",
        "\n",
        "##################################\n",
        "####### Part 2: Training #########\n",
        "\n",
        "class ZeroLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, lr_schedule):\n",
        "        super(ZeroLearningRateSchedule, self).__init__()\n",
        "        # Convert keys to integers\n",
        "        self.lr_schedule = {int(k): v for k, v in lr_schedule.items()}\n",
        "        self.lr_schedule_keys = sorted(self.lr_schedule)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        learning_rate = self.lr_schedule[self.lr_schedule_keys[0]]\n",
        "        for i in range(1, len(self.lr_schedule_keys)):\n",
        "            if step < self.lr_schedule_keys[i]:\n",
        "                break\n",
        "            learning_rate = self.lr_schedule[self.lr_schedule_keys[i]]\n",
        "        return learning_rate\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'lr_schedule': self.lr_schedule}\n",
        "\n",
        "\n",
        "def train_network(config: Config, storage: SharedStorage,\n",
        "                  replay_buffer: ReplayBuffer):\n",
        "  network = Network()\n",
        "  learning_rate_schedule = ZeroLearningRateSchedule(config.learning_rate_schedule)\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate_schedule, config.momentum)\n",
        "  for i in range(config.training_steps):\n",
        "    if i % config.checkpoint_interval == 0: # skip first checkpoint\n",
        "      print(\"Checkpoint: \", i // config.checkpoint_interval)\n",
        "      storage.save_network(i, network)\n",
        "    batch = replay_buffer.sample_batch()\n",
        "    update_weights(optimizer, network, batch, config.weight_decay)\n",
        "  storage.save_network(config.training_steps, network)\n",
        "\n",
        "def update_weights(optimizer: tf.keras.optimizers, network: Network, batch,\n",
        "                   weight_decay: float):\n",
        "  loss = 0\n",
        "  for image, (target_value, target_policy) in batch:\n",
        "    value, policy_logits = network.inference(image)\n",
        "    loss += (\n",
        "        tf.losses.mean_squared_error(value, target_value) +\n",
        "        tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=policy_logits, labels=target_policy))\n",
        "\n",
        "  for weights in network.get_weights():\n",
        "    loss += weight_decay * tf.nn.l2_loss(weights)\n",
        "\n",
        "  optimizer.minimize(loss)\n",
        "\n",
        "\n",
        "######### End Training ###########\n",
        "##################################\n",
        "\n",
        "\n",
        "def launch_job(f, *args):\n",
        "  f(*args)"
      ],
      "metadata": {
        "id": "Ou5wkaIzpvvk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir tmp\n",
        "!mkdir gifs\n",
        "!mkdir uci_pgn\n",
        "\n",
        "def svgs_to_pngs(svgs_data):\n",
        "  for i in range(len(svgs_data)):\n",
        "    cairosvg.svg2png(bytestring=svgs_data[i], write_to=f\"tmp/board_img_{i}.png\")\n",
        "\n",
        "\n",
        "def svgs_to_gif(svgs, game_idx):\n",
        "  svgs_to_pngs(svgs)\n",
        "\n",
        "  images = []\n",
        "  for i in range(len(svgs)):  # Assumes all PNGs are in the current directory\n",
        "      images.append(Image.open(f\"tmp/board_img_{i}.png\"))\n",
        "\n",
        "  images[0].save(f'gifs/game_{game_idx}_moves_{len(svgs)}.gif', save_all=True, append_images=images[1:], duration=200, loop=0)\n",
        "\n",
        "  return f'gifs/game_{game_idx}_moves_{len(svgs)}.gif'\n",
        "\n",
        "\n",
        "def display_svg(board: chess.Board, svgs, show_svg: bool=False):\n",
        "  boardsvg = chess.svg.board(board, size=350)\n",
        "  svgs.append(boardsvg)\n",
        "  if show_svg:\n",
        "    display(IPython.display.HTML(boardsvg))\n",
        "\n",
        "def visualize_game(config: Config, network: Network=None, show_svg: bool=True, store_gif: bool=False, game_idx: int=0):\n",
        "  num_simulations = config.num_simulations\n",
        "  num_sampling_moves = config.num_sampling_moves\n",
        "  softmax_temperature = config.softmax_temperature\n",
        "  if not network:\n",
        "    network = Network(True)\n",
        "    num_simulations = config.uniform_num_simulations\n",
        "    num_sampling_moves = config.uniform_num_sampling_moves\n",
        "    softmax_temperature = config.uniform_softmax_temperature\n",
        "\n",
        "  game = Game()\n",
        "  svgs = []\n",
        "\n",
        "  # initial board\n",
        "  display_svg(game.environment.board, svgs, show_svg)\n",
        "\n",
        "  while not game.terminal() and len(game.history) < config.max_moves:\n",
        "    action, root = run_mcts(config, game, network, num_simulations, num_sampling_moves, softmax_temperature)\n",
        "    game.apply(action)\n",
        "    display_svg(game.environment.board, svgs, show_svg)\n",
        "    game.store_search_statistics(root)\n",
        "\n",
        "  print(\"Game Ended with ply: \", game.environment.board.ply())\n",
        "\n",
        "  uci_pgn = \"\"\n",
        "  count = 2\n",
        "  for move in game.environment.board.move_stack:\n",
        "    if count % 10 == 0:\n",
        "      uci_pgn += \"\\n\"\n",
        "    if count % 2 == 0:\n",
        "      uci_pgn += f\"{count//2}.\"\n",
        "    uci_pgn += move.uci() + \" \"\n",
        "    count += 1\n",
        "\n",
        "  with open(f\"uci_pgn/game_{game_idx}_moves_{len(svgs)}.txt\", \"w\") as text_file:\n",
        "      text_file.write(uci_pgn)\n",
        "\n",
        "  if store_gif:\n",
        "    file_name = svgs_to_gif(svgs, game_idx)\n",
        "    print(\"Gif created at: \", file_name)\n",
        "\n",
        "  return game"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6Fhx_64I52B",
        "outputId": "0cbca9a9-09b8-4bc9-d8c7-c7b27ea5fe87"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘tmp’: File exists\n",
            "mkdir: cannot create directory ‘gifs’: File exists\n",
            "mkdir: cannot create directory ‘uci_pgn’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir games\n",
        "\n",
        "def dump_game(game, name):\n",
        "  with open(f'games/{name}.game', 'wb') as game_file:\n",
        "    pickle.dump(game, game_file)\n",
        "  return f'games/{name}.game'"
      ],
      "metadata": {
        "id": "892RJjOme9ZL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_visualize_game():\n",
        "  config = Config()\n",
        "\n",
        "  network = Network()\n",
        "\n",
        "  for i in range(1000):\n",
        "    game = visualize_game(config, network=network, show_svg=False, store_gif=False, game_idx=i)\n",
        "    name = dump_game(game, f\"uniform_{i}\")\n",
        "    print(\"Game stored at: \", name)"
      ],
      "metadata": {
        "id": "jIc4rw9TLZyN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir buffer\n",
        "\n",
        "config = Config()\n",
        "storage = SharedStorage()\n",
        "replay_buffer = ReplayBuffer(config)\n",
        "\n",
        "for i in range(config.window_size):\n",
        "  network = storage.lastest_network()\n",
        "  game = play_game(config)\n",
        "  replay_buffer.save_game(game)\n",
        "  if i % 100 == 0:\n",
        "    with open(f'buffer/uniform_step_0_sim.buffer', 'wb') as buffer:\n",
        "      pickle.dump(replay_buffer, buffer)\n",
        "    print(\"Checkpoint: \", i // 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "uqqVc_alhnOA",
        "outputId": "6bff9e3e-d2cb-4386-f818-685da7524b55"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint:  0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7130ff0af030>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-904caca2c724>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(config, network)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_mcts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_simulations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sampling_moves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_temperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_search_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-6682d45f4e25>\u001b[0m in \u001b[0;36mstore_search_statistics\u001b[0;34m(self, root)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mstore_search_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0msum_visits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit_count\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     self.child_visits.append([\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit_count\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msum_visits\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-6682d45f4e25>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0msum_visits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit_count\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     self.child_visits.append([\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit_count\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msum_visits\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     ])\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_network(config, storage, replay_buffer)replay_buffer.size()"
      ],
      "metadata": {
        "id": "yKzGA77G6h2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.training_steps"
      ],
      "metadata": {
        "id": "tsgLx3x98qnB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}