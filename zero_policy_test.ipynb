{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQWZvQ3u54LZKofS+Rm4WK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lunathanael/chessnn/blob/main/zero_policy_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {
        "id": "aEOVZHf_fRtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e9f623-3552-45ce-fda8-e5c04833c9f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chess in /usr/local/lib/python3.10/dist-packages (1.10.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install chess\n",
        "import chess\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from typing import List\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Add, Dense, Flatten\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(x, filters):\n",
        "    \"\"\"Create a residual block.\"\"\"\n",
        "    y = Conv2D(filters, kernel_size=3, padding='same')(x)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = ReLU()(y)\n",
        "    y = Conv2D(filters, kernel_size=3, padding='same')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Add()([y, x])\n",
        "    y = ReLU()(y)\n",
        "    return y\n",
        "\n",
        "def make_network():\n",
        "    # Input layer\n",
        "    input_layer = Input(shape=(8, 8, 20))  # 8x8 grid with 20 features per cell\n",
        "\n",
        "    # Body\n",
        "    x = Conv2D(256, kernel_size=3, padding='same', activation='relu')(input_layer)\n",
        "    x = BatchNormalization()(x)\n",
        "    # 19 residual blocks\n",
        "    for _ in range(19):\n",
        "        x = residual_block(x, 256)  # Each block has two convolutional layers with 256 filters, kernel size 3x3\n",
        "\n",
        "    # Policy Head\n",
        "    policy_head = Conv2D(256, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    policy_head = BatchNormalization()(policy_head)\n",
        "    policy_head = Conv2D(73, kernel_size=1)(policy_head)  # Output 73 policies\n",
        "\n",
        "    # Value Head\n",
        "    value_head = Conv2D(1, kernel_size=1, activation='relu')(x)\n",
        "    value_head = BatchNormalization()(value_head)\n",
        "    value_head = Flatten()(value_head)\n",
        "    value_head = Dense(256, activation='relu')(value_head)\n",
        "    value_head = Dense(1, activation='tanh')(value_head)  # Output single value\n",
        "\n",
        "    # Create the model\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=[value_head, policy_head])\n",
        "    return model\n",
        "\n",
        "\n",
        "def ConstantPolicyHead(shape):\n",
        "    # Custom layer for policy head\n",
        "    class CPHead(tf.keras.layers.Layer):\n",
        "        def call(self, inputs):\n",
        "            return tf.constant(1/73, shape=shape)\n",
        "\n",
        "    return CPHead()\n",
        "\n",
        "def ConstantValueHead():\n",
        "    # Custom layer for value head\n",
        "    class CVHead(tf.keras.layers.Layer):\n",
        "        def call(self, inputs):\n",
        "            return tf.constant(0.5, shape=(1,))\n",
        "\n",
        "    return CVHead()\n",
        "\n",
        "def make_uniform_network():\n",
        "    # Input layer\n",
        "    input_layer = Input(shape=(8, 8, 20))\n",
        "\n",
        "    # Body\n",
        "    x = Conv2D(256, kernel_size=3, padding='same')(input_layer)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    for _ in range(19):\n",
        "        x = residual_block(x, 256)\n",
        "\n",
        "    # Modified Policy Head for uniform output\n",
        "    policy_head = ConstantPolicyHead((8, 8, 73))(x)\n",
        "\n",
        "    # Modified Value Head for constant output\n",
        "    value_head = ConstantValueHead()(x)\n",
        "\n",
        "    # Create the model with constant outputs\n",
        "    model = Model(inputs=input_layer, outputs=[value_head, policy_head])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ABgGpmx0fWGi"
      },
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helpers"
      ],
      "metadata": {
        "id": "P3mUyAMlzxsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    ### Self-Play\n",
        "    self.num_actors = 5000\n",
        "\n",
        "    self.num_sampling_moves = 30\n",
        "    self.max_moves = 512  # for chess and shogi, 722 for Go.\n",
        "    self.num_simulations = 800\n",
        "\n",
        "    # Root prior exploration noise.\n",
        "    self.root_dirichlet_alpha = 0.3  # for chess, 0.03 for Go and 0.15 for shogi.\n",
        "    self.root_exploration_fraction = 0.25\n",
        "\n",
        "    # UCB formula\n",
        "    self.pb_c_base = 19652\n",
        "    self.pb_c_init = 1.25\n",
        "\n",
        "    ### Training\n",
        "    self.training_steps = int(700e3)\n",
        "    self.checkpoint_interval = int(1e3)\n",
        "    self.window_size = int(1e6)\n",
        "    self.batch_size = 4096\n",
        "\n",
        "    self.weight_decay = 1e-4\n",
        "    self.momentum = 0.9\n",
        "    # Schedule for chess and shogi, Go starts at 2e-2 immediately.\n",
        "    self.learning_rate_schedule = {\n",
        "        0: 2e-1,\n",
        "        100e3: 2e-2,\n",
        "        300e3: 2e-3,\n",
        "        500e3: 2e-4\n",
        "    }"
      ],
      "metadata": {
        "id": "t32QDQxky6Yn"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Node(object):\n",
        "\n",
        "  def __init__(self, prior: float):\n",
        "    self.visit_count = 0\n",
        "    self.to_play = -1\n",
        "    self.prior = prior\n",
        "    self.value_sum = 0\n",
        "    self.children = {}\n",
        "\n",
        "  def expanded(self):\n",
        "    return len(self.children) > 0\n",
        "\n",
        "  def value(self):\n",
        "    if self.visit_count == 0:\n",
        "      return 0\n",
        "    return self.value_sum / self.visit_count\n"
      ],
      "metadata": {
        "id": "Lvmt70d1y8HR"
      },
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fen_to_repr(fen, repeats):\n",
        "    # Split the FEN string to get the relevant parts\n",
        "    parts = fen.split(' ')\n",
        "    board_fen, player, castling, _, halfmove, fullmove = parts[:6]\n",
        "\n",
        "    # 12 pieces, 4 cf, col, rep, half, total\n",
        "    board = np.zeros((8, 8, 12 + 4 + 1 + 1 + 1 + 1), dtype=float)\n",
        "\n",
        "    # Define piece order and mapping to layers\n",
        "    piece_map = {'p': 0, 'n': 1, 'b': 2, 'r': 3, 'q': 4, 'k': 5,\n",
        "                 'P': 6, 'N': 7, 'B': 8, 'R': 9, 'Q': 10, 'K': 11}\n",
        "\n",
        "    # Fill the board with pieces\n",
        "    row = 0\n",
        "    col = 0\n",
        "    for char in board_fen:\n",
        "        if char.isdigit():\n",
        "            col += int(char)\n",
        "        elif char == '/':\n",
        "            row += 1\n",
        "            col = 0\n",
        "        else:\n",
        "            board[row, col, piece_map[char]] = 1\n",
        "            col += 1\n",
        "\n",
        "    # Castling rights encoded in four binary planes\n",
        "    board[:, :, 12] = 1 if 'K' in castling else 0  # White kingside\n",
        "    board[:, :, 13] = 1 if 'Q' in castling else 0  # White queenside\n",
        "    board[:, :, 14] = 1 if 'k' in castling else 0  # Black kingside\n",
        "    board[:, :, 15] = 1 if 'q' in castling else 0  # Black queenside\n",
        "\n",
        "    # Player color (1 for black, 0 for white)\n",
        "    board[:, :, 16] = 1 if player == 'b' else 0\n",
        "\n",
        "    # Position repetitions\n",
        "    board[:, :, 17] = repeats\n",
        "\n",
        "    board[:, :, 18] = float(halfmove)\n",
        "\n",
        "    # Move number (as a real value)\n",
        "    board[:, :, 19] = float(fullmove)\n",
        "\n",
        "    return board"
      ],
      "metadata": {
        "id": "1LZ0finYdR6k"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def square_to_coord(square):\n",
        "  #       r           c\n",
        "  return (square // 8, square % 8)\n",
        "\n",
        "piece_map = {\n",
        "        'P' : 0, 'N' : 1, 'B' : 2, 'R' : 3, 'Q' : 4, 'K' : 5,\n",
        "        'p' : 6, 'n' : 7, 'b' : 8, 'r' : 9, 'q' : 10, 'k' : 11,\n",
        "}\n",
        "\n",
        "\n",
        "class Environment(object):\n",
        "\n",
        "  def __init__(self, env=None):\n",
        "    if env == None:\n",
        "      self.board = chess.Board(chess.STARTING_FEN)\n",
        "      self.board_history = []\n",
        "      self.repetitions = {}\n",
        "\n",
        "      self.update_history()\n",
        "    else:\n",
        "      self.board = chess.copy(env.board)\n",
        "      self.board_history = env.board_history.copy()\n",
        "      self.repetitions = env.repetitions.copy()\n",
        "\n",
        "  def is_terminal(self):\n",
        "    return self.board.is_game_over()\n",
        "\n",
        "  def terminal_value(self, to_play):\n",
        "    if self.board.is_checkmate():\n",
        "      color = self.board.outcome.winner\n",
        "      color = (color == chess.BLACK)\n",
        "      return color == to_play\n",
        "    else:\n",
        "      return 0.5\n",
        "\n",
        "  def generate_legal_moves(self):\n",
        "    return self.board.generate_legal_moves()\n",
        "\n",
        "  def encode_action(self, move): #12 piece types, 4 flags, 2 coords\n",
        "    action = np.zeros(shape=(8, 8, 73), dtype=int)\n",
        "\n",
        "    piece = self.board.piece_at(move.from_square)\n",
        "    piece = piece_map[str(piece)]\n",
        "    fr, fc = square_to_coord(move.from_square)\n",
        "    tr, tc = square_to_coord(move.to_square)\n",
        "\n",
        "    action[:,:,piece] = 1\n",
        "\n",
        "    action[fr, fc, 12] = 1\n",
        "    action[tr, tc, 13] = 1\n",
        "\n",
        "    if move.promotion:\n",
        "      pp = move.promotion + 12 # 5,4,3,2\n",
        "      action[:,:, pp] = 1\n",
        "\n",
        "    return action\n",
        "\n",
        "  def generate_legal_actions(self):\n",
        "    moves = self.generate_legal_moves()\n",
        "    actions = [self.encode_action(move) for move in moves]\n",
        "    return np.array(actions)\n",
        "\n",
        "  def update_history(self):\n",
        "    fen = self.board.fen()\n",
        "    s_fen = fen.split(' ')\n",
        "    cs_fen = ' '.join(s_fen[:-2])\n",
        "\n",
        "    if cs_fen in self.repetitions:\n",
        "      self.repetitions[cs_fen] += 1\n",
        "      repeats = self.repetitions[cs_fen]\n",
        "    else:\n",
        "      self.repetitions[cs_fen] = 1\n",
        "      repeats = 1\n",
        "\n",
        "    halfmove_clock = self.board.halfmove_clock\n",
        "    self.board_history.append((fen, repeats))\n",
        "\n",
        "  def make_image(self, state_index=-1):\n",
        "    fen, repeats= self.board_history[state_index]\n",
        "\n",
        "    repr = fen_to_repr(fen, repeats)\n",
        "    return repr"
      ],
      "metadata": {
        "id": "3FE-c9peKBP2"
      },
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Game(object):\n",
        "\n",
        "  def __init__(self, history=None, environment=None):\n",
        "    self.history = history or []\n",
        "    self.child_visits = []\n",
        "    self.num_actions = 4672  # action space size for chess\n",
        "    self.environment = Environment(environment)\n",
        "\n",
        "  def terminal(self):\n",
        "    # Game specific termination rules.\n",
        "    return self.environment.is_terminal()\n",
        "\n",
        "  def terminal_value(self, to_play):\n",
        "    # Game specific value.\n",
        "    return self.environment.terminal_value()\n",
        "\n",
        "  def legal_actions(self): #TODO\n",
        "    # Game specific calculation of legal actions.\n",
        "    return self.environment.generate_legal_actions()\n",
        "\n",
        "  def clone(self):\n",
        "    return Game(list(self.history), self.environment)\n",
        "\n",
        "  def apply(self, action):\n",
        "    self.history.append(action)\n",
        "\n",
        "  def store_search_statistics(self, root):\n",
        "    sum_visits = sum(child.visit_count for child in root.children.itervalues())\n",
        "    self.child_visits.append([\n",
        "        root.children[a].visit_count / sum_visits if a in root.children else 0\n",
        "        for a in range(self.num_actions)\n",
        "    ])\n",
        "\n",
        "  def make_image(self, state_index: int): #TODO\n",
        "    # Game specific feature planes.\n",
        "    return self.environment.make_image(state_index)\n",
        "\n",
        "  def make_target(self, state_index: int):\n",
        "    return (self.terminal_value(state_index % 2),\n",
        "            self.child_visits[state_index])\n",
        "\n",
        "  def to_play(self):\n",
        "    return len(self.history) % 2\n"
      ],
      "metadata": {
        "id": "rSCKGOM9zjwa"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, config: Config):\n",
        "    self.window_size = config.window_size\n",
        "    self.batch_size = config.batch_size\n",
        "    self.buffer = []\n",
        "\n",
        "  def save_game(self, game):\n",
        "    if len(self.buffer) > self.window_size:\n",
        "      self.buffer.pop(0)\n",
        "    self.buffer.append(game)\n",
        "\n",
        "  def sample_batch(self):\n",
        "    # Sample uniformly across positions.\n",
        "    move_sum = float(sum(len(g.history) for g in self.buffer))\n",
        "    games = np.random.choice(\n",
        "        self.buffer,\n",
        "        size=self.batch_size,\n",
        "        p=[len(g.history) / move_sum for g in self.buffer])\n",
        "    game_pos = [(g, np.random.randint(len(g.history))) for g in games]\n",
        "    return [(g.make_image(i), g.make_target(i)) for (g, i) in game_pos]"
      ],
      "metadata": {
        "id": "En06LbK6zoRk"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(object):\n",
        "\n",
        "  def __init__(self, uniform_model: bool=False):\n",
        "    if uniform_model:\n",
        "      self.model = make_uniform_network()\n",
        "    else:\n",
        "      self.model = make_network()\n",
        "\n",
        "  def inference(self, image):\n",
        "      # Run the neural network model to get predictions\n",
        "      value, policy_logits = self.model.predict(np.array([image]))\n",
        "\n",
        "      value = value[0] # The value output is a scalar representing the predicted game outcome\n",
        "\n",
        "      return value, np.array(policy_logits)\n",
        "\n",
        "\n",
        "  def get_weights(self): #TODO\n",
        "    # Returns the weights of this network.\n",
        "    return self.model.get_weights()"
      ],
      "metadata": {
        "id": "HPr_d9UOzqBG"
      },
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SharedStorage(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._networks = {}\n",
        "\n",
        "  def latest_network(self) -> Network:\n",
        "    if self._networks:\n",
        "      return self._networks[max(self._networks.iterkeys())]\n",
        "    else:\n",
        "      return Network(True)  # policy -> uniform, value -> 0.5\n",
        "\n",
        "  def save_network(self, step: int, network: Network):\n",
        "    self._networks[step] = network\n"
      ],
      "metadata": {
        "id": "6UsTnTQKzreQ"
      },
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AlphaZero training is split into two independent parts: Network training and\n",
        "# self-play data generation.\n",
        "# These two parts only communicate by transferring the latest network checkpoint\n",
        "# from the training to the self-play, and the finished games from the self-play\n",
        "# to the training.\n",
        "def zero(config: Config):\n",
        "  storage = SharedStorage()\n",
        "  replay_buffer = ReplayBuffer(config)\n",
        "\n",
        "  for i in range(config.num_actors):\n",
        "    launch_job(run_selfplay, config, storage, replay_buffer)\n",
        "\n",
        "  train_network(config, storage, replay_buffer)\n",
        "\n",
        "  return storage.latest_network()\n",
        "\n"
      ],
      "metadata": {
        "id": "tV9QkBmLBKT9"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "####### Part 1: Self-Play ########\n",
        "\n",
        "\n",
        "# Each self-play job is independent of all others; it takes the latest network\n",
        "# snapshot, produces a game and makes it available to the training job by\n",
        "# writing it to a shared replay buffer.\n",
        "def run_selfplay(config: Config, storage: SharedStorage,\n",
        "                 replay_buffer: ReplayBuffer):\n",
        "  while True:\n",
        "    network = storage.latest_network()\n",
        "    game = play_game(config, network)\n",
        "    replay_buffer.save_game(game)\n",
        "\n",
        "\n",
        "# Each game is produced by starting at the initial board position, then\n",
        "# repeatedly executing a Monte Carlo Tree Search to generate moves until the end\n",
        "# of the game is reached.\n",
        "def play_game(config: Config, network: Network):\n",
        "  game = Game()\n",
        "  while not game.terminal() and len(game.history) < config.max_moves:\n",
        "    action, root = run_mcts(config, game, network)\n",
        "    game.apply(action)\n",
        "    game.store_search_statistics(root)\n",
        "  return game\n",
        "\n",
        "\n",
        "# Core Monte Carlo Tree Search algorithm.\n",
        "# To decide on an action, we run N simulations, always starting at the root of\n",
        "# the search tree and traversing the tree according to the UCB formula until we\n",
        "# reach a leaf node.\n",
        "def run_mcts(config: AlphaZeroConfig, game: Game, network: Network):\n",
        "  root = Node(0)\n",
        "  evaluate(root, game, network)\n",
        "  add_exploration_noise(config, root)\n",
        "\n",
        "  for _ in range(config.num_simulations):\n",
        "    node = root\n",
        "    scratch_game = game.clone()\n",
        "    search_path = [node]\n",
        "\n",
        "    while node.expanded():\n",
        "      action, node = select_child(config, node)\n",
        "      scratch_game.apply(action)\n",
        "      search_path.append(node)\n",
        "\n",
        "    value = evaluate(node, scratch_game, network)\n",
        "    backpropagate(search_path, value, scratch_game.to_play())\n",
        "  return select_action(config, game, root), root\n",
        "\n",
        "\n",
        "\n",
        "def softmax_sample(visit_counts, temperature=10.0):\n",
        "    counts, actions = zip(*visit_counts)\n",
        "    # Apply softmax with temperature\n",
        "    counts = np.array(counts)\n",
        "    counts = counts / temperature  # Apply temperature scaling\n",
        "    softmax_probs = np.exp(counts) / sum(np.exp(counts))\n",
        "    # Sample an action based on the softmax probabilities\n",
        "    action = np.random.choice(actions, p=softmax_probs)\n",
        "    return softmax_probs, action\n",
        "\n",
        "\n",
        "def select_action(config: AlphaZeroConfig, game: Game, root: Node):\n",
        "  visit_counts = [(child.visit_count, action)\n",
        "                  for action, child in root.children.iteritems()]\n",
        "  if len(game.history) < config.num_sampling_moves:\n",
        "    _, action = softmax_sample(visit_counts)\n",
        "  else:\n",
        "    _, action = max(visit_counts)\n",
        "  return action\n",
        "\n",
        "\n",
        "# Select the child with the highest UCB score.\n",
        "def select_child(config: AlphaZeroConfig, node: Node):\n",
        "  _, action, child = max((ucb_score(config, node, child), action, child)\n",
        "                         for action, child in node.children.iteritems())\n",
        "  return action, child\n",
        "\n",
        "\n",
        "# The score for a node is based on its value, plus an exploration bonus based on\n",
        "# the prior.\n",
        "def ucb_score(config: AlphaZeroConfig, parent: Node, child: Node):\n",
        "  pb_c = math.log((parent.visit_count + config.pb_c_base + 1) /\n",
        "                  config.pb_c_base) + config.pb_c_init\n",
        "  pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "  prior_score = pb_c * child.prior\n",
        "  value_score = child.value()\n",
        "  return prior_score + value_score\n",
        "\n",
        "\n",
        "# We use the neural network to obtain a value and policy prediction.\n",
        "def evaluate(node: Node, game: Game, network: Network):\n",
        "  value, policy_logits = network.inference(game.make_image(-1))\n",
        "\n",
        "  # Expand the node.\n",
        "  node.to_play = game.to_play()\n",
        "\n",
        "  policy = {a: math.exp(policy_logits[a]) for a in game.legal_actions()}\n",
        "  policy_sum = sum(policy.itervalues())\n",
        "  for action, p in policy.iteritems():\n",
        "    node.children[action] = Node(p / policy_sum)\n",
        "  return value\n",
        "\n",
        "\n",
        "# At the end of a simulation, we propagate the evaluation all the way up the\n",
        "# tree to the root.\n",
        "def backpropagate(search_path: List[Node], value: float, to_play):\n",
        "  for node in search_path:\n",
        "    node.value_sum += value if node.to_play == to_play else (1 - value)\n",
        "    node.visit_count += 1\n",
        "\n",
        "\n",
        "# At the start of each search, we add dirichlet noise to the prior of the root\n",
        "# to encourage the search to explore new actions.\n",
        "def add_exploration_noise(config: AlphaZeroConfig, node: Node):\n",
        "  actions = node.children.keys()\n",
        "  noise = numpy.random.gamma(config.root_dirichlet_alpha, 1, len(actions))\n",
        "  frac = config.root_exploration_fraction\n",
        "  for a, n in zip(actions, noise):\n",
        "    node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "######### End Self-Play ##########\n",
        "##################################\n",
        "\n",
        "##################################\n",
        "####### Part 2: Training #########\n",
        "\n",
        "\n",
        "def train_network(config: AlphaZeroConfig, storage: SharedStorage,\n",
        "                  replay_buffer: ReplayBuffer):\n",
        "  network = Network()\n",
        "  optimizer = tf.keras.optimizers.SGD(config.learning_rate_schedule,\n",
        "                                         config.momentum)\n",
        "  for i in range(config.training_steps):\n",
        "    if i % config.checkpoint_interval == 0:\n",
        "      storage.save_network(i, network)\n",
        "    batch = replay_buffer.sample_batch()\n",
        "    update_weights(optimizer, network, batch, config.weight_decay)\n",
        "  storage.save_network(config.training_steps, network)\n",
        "\n",
        "def update_weights(optimizer: tf.keras.optimizers, network: Network, batch,\n",
        "                   weight_decay: float):\n",
        "  loss = 0\n",
        "  for image, (target_value, target_policy) in batch:\n",
        "    value, policy_logits = network.inference(image)\n",
        "    loss += (\n",
        "        tf.losses.mean_squared_error(value, target_value) +\n",
        "        tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=policy_logits, labels=target_policy))\n",
        "\n",
        "  for weights in network.get_weights():\n",
        "    loss += weight_decay * tf.nn.l2_loss(weights)\n",
        "\n",
        "  optimizer.minimize(loss)\n",
        "\n",
        "\n",
        "######### End Training ###########\n",
        "##################################\n",
        "\n",
        "\n",
        "def launch_job(f, *args):\n",
        "  f(*args)"
      ],
      "metadata": {
        "id": "Ou5wkaIzpvvk"
      },
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "network_1 = zero(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "tsgLx3x98qnB",
        "outputId": "069a7b72-98ef-497e-b024-878b9e16b94b"
      },
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 801ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "only size-1 arrays can be converted to Python scalars",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-344-fded5ef91511>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnetwork_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-338-c5b1727b6988>\u001b[0m in \u001b[0;36mzero\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mlaunch_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_selfplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-339-95b813d2b559>\u001b[0m in \u001b[0;36mlaunch_job\u001b[0;34m(f, *args)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlaunch_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m   \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-339-95b813d2b559>\u001b[0m in \u001b[0;36mrun_selfplay\u001b[0;34m(config, storage, replay_buffer)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-339-95b813d2b559>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(config, network)\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_moves\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_mcts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_search_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-339-95b813d2b559>\u001b[0m in \u001b[0;36mrun_mcts\u001b[0;34m(config, game, network)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_mcts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAlphaZeroConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0madd_exploration_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-339-95b813d2b559>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(node, game, network)\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_play\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegal_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m   \u001b[0mpolicy_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitervalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-339-95b813d2b559>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_play\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegal_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m   \u001b[0mpolicy_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitervalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  gs = Game()\n",
        "  nn = Network(True)\n",
        "\n",
        "  value, policy_logits = nn.inference(gs.make_image(-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcjR2GstypEe",
        "outputId": "fb1d4497-ffa2-4130-c799-b57335fe5a1b"
      },
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 680ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for a in gs.legal_actions():\n",
        "  print((policy_logits(a)).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "oC36LFwV0OrI",
        "outputId": "fb92e54f-1775-45c2-e946-7ad40de9865e"
      },
      "execution_count": 352,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'numpy.ndarray' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-352-1240a8ee5955>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegal_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
          ]
        }
      ]
    }
  ]
}