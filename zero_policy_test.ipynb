{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lunathanael/chessnn/blob/main/zero_policy_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEOVZHf_fRtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa91273-3cd0-4592-f831-6ac3b2b55107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chess\n",
            "  Downloading chess-1.10.0-py3-none-any.whl (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/154.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install chess\n",
        "import chess\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from typing import List\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Add, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "from threading import Thread, Event\n",
        "\n",
        "!pip install cairosvg\n",
        "import chess.svg\n",
        "import IPython\n",
        "from IPython.display import SVG, display\n",
        "\n",
        "import cairosvg\n",
        "from PIL import Image\n",
        "\n",
        "import pickle\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(x, filters):\n",
        "    \"\"\"Create a residual block.\"\"\"\n",
        "    y = Conv2D(filters, kernel_size=3, padding='same')(x)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = ReLU()(y)\n",
        "    y = Conv2D(filters, kernel_size=3, padding='same')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Add()([y, x])\n",
        "    y = ReLU()(y)\n",
        "    return y\n",
        "\n",
        "def make_network():\n",
        "    # Input layer\n",
        "    input_layer = Input(shape=(8, 8, 20))  # 8x8 grid with 20 features per cell\n",
        "\n",
        "    # Body\n",
        "    x = Conv2D(256, kernel_size=3, padding='same', activation='relu')(input_layer)\n",
        "    x = BatchNormalization()(x)\n",
        "    # 19 residual blocks\n",
        "    for _ in range(19):\n",
        "        x = residual_block(x, 256)  # Each block has two convolutional layers with 256 filters, kernel size 3x3\n",
        "\n",
        "    # Policy Head\n",
        "    policy_head = Conv2D(256, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    policy_head = BatchNormalization()(policy_head)\n",
        "    policy_head = Conv2D(73, kernel_size=1)(policy_head)  # Output 73 policies\n",
        "\n",
        "    # Value Head\n",
        "    value_head = Conv2D(1, kernel_size=1, activation='relu')(x)\n",
        "    value_head = BatchNormalization()(value_head)\n",
        "    value_head = Flatten()(value_head)\n",
        "    value_head = Dense(256, activation='relu')(value_head)\n",
        "    value_head = Dense(1, activation='tanh')(value_head)  # Output single value\n",
        "\n",
        "    # Create the model\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=[value_head, policy_head])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "class make_uniform_network():\n",
        "  def __init__(self):\n",
        "    self._sval =  np.full((1,1,), 0.5)\n",
        "    self._spolicy = array2 = np.full((1, 8, 8, 73), 1/73)\n",
        "\n",
        "    self._val =  np.full((1,1,), 0.5)\n",
        "    self._policy = array2 = np.full((1, 8, 8, 73), 1/73)\n",
        "\n",
        "\n",
        "  def predict(self, input, verbose=0):\n",
        "    repeat_times = input.shape[0]\n",
        "    if repeat_times != 1:\n",
        "      # Repeat _val and _policy\n",
        "      repeated_val = np.repeat(self._val, repeat_times, axis=0)\n",
        "      repeated_policy = np.repeat(self._policy, repeat_times, axis=0)\n",
        "\n",
        "      return repeated_val, repeated_policy\n",
        "    return self._sval, self._spolicy"
      ],
      "metadata": {
        "id": "ABgGpmx0fWGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helpers"
      ],
      "metadata": {
        "id": "P3mUyAMlzxsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    ### Self-Play\n",
        "    self.num_actors = 5 # not enough ram <- 5000\n",
        "\n",
        "    self.num_sampling_moves = 30\n",
        "    self.max_moves = 512  # for chess and shogi, 722 for Go.\n",
        "\n",
        "    self.num_simulations = 100 #was 800, testing with 31\n",
        "\n",
        "    self.random_action = False # pick random node to explore\n",
        "    self.uniform_num_simulations = 800 # avg number of legal moves is 31\n",
        "    self.uniform_num_sampling_moves = 30 # all are sampled from softmax\n",
        "    self.uniform_softmax_temperature = 10\n",
        "\n",
        "    # Root prior exploration noise.\n",
        "    self.root_dirichlet_alpha = 0.3  # for chess, 0.03 for Go and 0.15 for shogi.\n",
        "    self.root_exploration_fraction = 0.25\n",
        "\n",
        "    # Softmax function\n",
        "    self.softmax_temperature = 10 # reduce later on when fine-tuning, choose moves that it believes are more likely to be successful\n",
        "\n",
        "    # UCB formula\n",
        "    self.pb_c_base = 19652\n",
        "    self.pb_c_init = 1.25\n",
        "\n",
        "    ### Training\n",
        "    self.training_steps = int(700e3) # 700,000 take too long?\n",
        "    self.checkpoint_interval = int(1e3)\n",
        "    self.window_size = int(1e6)\n",
        "    self.batch_size = 32 # 4096 -> 32 had best performance for me\n",
        "\n",
        "    self.weight_decay = 1e-4\n",
        "    self.momentum = 0.9\n",
        "    self.clipnorm = 0.05 # set to None for no gradient clipping\n",
        "    # Schedule for chess and shogi, Go starts at 2e-2 immediately.\n",
        "    self.learning_rate_schedule = {\n",
        "        0: 2e-1,\n",
        "        100e3: 2e-2,\n",
        "        300e3: 2e-3,\n",
        "        500e3: 2e-4\n",
        "    }"
      ],
      "metadata": {
        "id": "t32QDQxky6Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Node(object):\n",
        "\n",
        "  def __init__(self, prior: float):\n",
        "    self.visit_count = 0\n",
        "    self.to_play = -1\n",
        "    self.prior = prior\n",
        "    self.value_sum = 0\n",
        "    self.children = {}\n",
        "\n",
        "  def expanded(self):\n",
        "    return len(self.children) > 0\n",
        "\n",
        "  def value(self):\n",
        "    if self.visit_count == 0:\n",
        "      return 0\n",
        "    return self.value_sum / self.visit_count\n"
      ],
      "metadata": {
        "id": "Lvmt70d1y8HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fen_to_repr(fen, repeats):\n",
        "    # Split the FEN string to get the relevant parts\n",
        "    parts = fen.split(' ')\n",
        "    board_fen, player, castling, _, halfmove, fullmove = parts[:6]\n",
        "\n",
        "    # 12 pieces, 4 cf, col, rep, half, total\n",
        "    board = np.zeros((8, 8, 12 + 4 + 1 + 1 + 1 + 1), dtype=float)\n",
        "\n",
        "    # Define piece order and mapping to layers\n",
        "    piece_map = {'p': 0, 'n': 1, 'b': 2, 'r': 3, 'q': 4, 'k': 5,\n",
        "                 'P': 6, 'N': 7, 'B': 8, 'R': 9, 'Q': 10, 'K': 11}\n",
        "\n",
        "    # Fill the board with pieces\n",
        "    row = 0\n",
        "    col = 0\n",
        "    for char in board_fen:\n",
        "        if char.isdigit():\n",
        "            col += int(char)\n",
        "        elif char == '/':\n",
        "            row += 1\n",
        "            col = 0\n",
        "        else:\n",
        "            board[row, col, piece_map[char]] = 1\n",
        "            col += 1\n",
        "\n",
        "    # Castling rights encoded in four binary planes\n",
        "    board[:, :, 12] = 1 if 'K' in castling else 0  # White kingside\n",
        "    board[:, :, 13] = 1 if 'Q' in castling else 0  # White queenside\n",
        "    board[:, :, 14] = 1 if 'k' in castling else 0  # Black kingside\n",
        "    board[:, :, 15] = 1 if 'q' in castling else 0  # Black queenside\n",
        "\n",
        "    # Player color (1 for black, 0 for white)\n",
        "    board[:, :, 16] = 1 if player == 'b' else 0\n",
        "\n",
        "    # Position repetitions\n",
        "    board[:, :, 17] = repeats\n",
        "\n",
        "    board[:, :, 18] = float(halfmove)\n",
        "\n",
        "    # Move number (as a real value)\n",
        "    board[:, :, 19] = float(fullmove)\n",
        "\n",
        "    return board"
      ],
      "metadata": {
        "id": "1LZ0finYdR6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def square_to_coord(square):\n",
        "  #       r           c\n",
        "  return (square // 8, square % 8)\n",
        "\n",
        "knight_moves = [\n",
        "    (1, 2), (1, -2),\n",
        "    (2, 1), (2, -1),\n",
        "    (-1, 2), (-1, -2),\n",
        "    (-2, 1), (-2, -1)\n",
        "]\n",
        "\n",
        "promotion_pieces = \"nbr\"\n",
        "rows = \"abcdefgh\"\n",
        "\n",
        "queen_moves = [\n",
        "    (1, 1), (1, -1), (1, 0),\n",
        "    (-1, 1), (-1, -1), (-1, 0),\n",
        "    (0, 1), (0, -1)\n",
        "]\n",
        "\n",
        "def coord_to_uci(fr, fc, tr, tc):\n",
        "  return rows[fc] + str(fr + 1) + rows[tc] + str(tr + 1)\n",
        "\n",
        "def action_tuple_to_index(action):\n",
        "  fr, fc, plane = action\n",
        "  dim2, dim3 = 8, 73\n",
        "  return (fr * dim2 * dim3) + (fc * dim3) + plane\n",
        "\n",
        "def decode_action(action):\n",
        "\n",
        "  #piece = self.board.piece_at(move.from_square)\n",
        "  #fr, fc = square_to_coord(move.from_square)\n",
        "  #tr, tc = square_to_coord(move.to_square)\n",
        "  fr = action[0]\n",
        "  fc = action[1]\n",
        "\n",
        "  if action[2] < 9:\n",
        "    piece_idx = action[2] // 3\n",
        "    move_idx = action[2] % 3\n",
        "    if fr == 1: # black promo\n",
        "      tr = 0\n",
        "    else:\n",
        "      tr = 7\n",
        "    if move_idx == 0:\n",
        "      tc = fc\n",
        "    elif move_idx == 1:\n",
        "      tc = fc - 1\n",
        "    else:\n",
        "      tc = fc + 1\n",
        "    return coord_to_uci(fr, fc, tr, tc) + promotion_pieces[piece_idx]\n",
        "\n",
        "  if action[2] < 17:\n",
        "    tr = fr + knight_moves[action[2] - 9][0]\n",
        "    tc = fc + knight_moves[action[2] - 9][1]\n",
        "  else:\n",
        "    dist = (action[2] - 17) // 8 + 1\n",
        "    status = (action[2] - 17) % 8\n",
        "    tr = fr + (queen_moves[status][0] * dist)\n",
        "    tc = fc + (queen_moves[status][1] * dist)\n",
        "\n",
        "  return coord_to_uci(fr, fc, tr, tc)\n",
        "\n",
        "\n",
        "class Environment(object):\n",
        "\n",
        "  def __init__(self, env=None):\n",
        "    if env == None:\n",
        "      self.board = chess.Board(chess.STARTING_FEN)\n",
        "      self.board_history = []\n",
        "      self.repetitions = {}\n",
        "\n",
        "      self.update_history()\n",
        "    else:\n",
        "      self.board = chess.Board.copy(env.board)\n",
        "      self.board_history = env.board_history.copy()\n",
        "      self.repetitions = env.repetitions.copy()\n",
        "\n",
        "  def is_terminal(self):\n",
        "    return self.board.outcome(claim_draw=True) != None # claim draws\n",
        "\n",
        "  def terminal_value(self, to_play: int) -> int: # THIS FUNCTION COULD BE VERY INCORRECT, ASSUMING CLASSIC MCTS\n",
        "    # outcome = self.board.outcome(claim_draw=True)\n",
        "    # if not outcome:\n",
        "    #   return 0 # ERROR, SHOULD NEVER BE CALLED LIKE SO.\n",
        "    outcome = self.board.outcome(claim_draw=True)\n",
        "\n",
        "    if not outcome:\n",
        "      if len(self.board.history) < 512: # config.max_moves: for chess\n",
        "        print(f\"Outcome not found for game with {len(self.board.history)} moves\")\n",
        "      return 0.5\n",
        "\n",
        "    winner = outcome.winner\n",
        "\n",
        "    if winner != None:      # -> white node  ->white node\n",
        "      if winner == to_play: # white_won(True) and black(1) to play or other way,\n",
        "        return 1\n",
        "      return 0\n",
        "\n",
        "    return 0.5\n",
        "\n",
        "  def generate_legal_moves(self):\n",
        "    return self.board.generate_legal_moves()\n",
        "\n",
        "  def encode_action(self, move): #12 piece types, 4 flags, 2 coords\n",
        "\n",
        "    piece = self.board.piece_at(move.from_square)\n",
        "    fr, fc = square_to_coord(move.from_square)\n",
        "    tr, tc = square_to_coord(move.to_square)\n",
        "\n",
        "    if move.promotion and move.promotion != 5: # 9 planes\n",
        "      # move.promotion -> 2 : N, 3 : B, 4 : R, 5 : Q\n",
        "      piece_idx = move.promotion - 2\n",
        "      if fc == tc:\n",
        "        move_idx = 0\n",
        "      elif tc < fc: # left from white perspective\n",
        "        move_idx = 1\n",
        "      else:\n",
        "        move_idx = 2\n",
        "      plane = piece_idx * 3 + move_idx\n",
        "\n",
        "    elif str(piece).lower() == 'n': # 8 planes\n",
        "      for idx in range(len(knight_moves)):\n",
        "        if fr + knight_moves[idx][0] == tr and fc + knight_moves[idx][1] == tc:\n",
        "          plane = idx + 9\n",
        "          break\n",
        "\n",
        "    else: # 7 * 8 planes\n",
        "      if fr < tr: # moving forward\n",
        "        row_status = 0\n",
        "      elif fr > tr: # back\n",
        "        row_status = 1\n",
        "      else: # horizontal movement\n",
        "        row_status = 2\n",
        "\n",
        "      if fc < tc: # moving right\n",
        "        col_status = 0\n",
        "      elif fc > tc: # left\n",
        "        col_status = 1\n",
        "      else: # vertical movement\n",
        "        col_status = 2\n",
        "\n",
        "      # row status and col status should never both be 2, means something stinky\n",
        "\n",
        "      dist = max(abs(fr - tr), abs(fc - tc)) - 1\n",
        "      plane = dist * 8 + (row_status * 3 + col_status) + 17\n",
        "\n",
        "    # if decode_action((fr, fc, plane)) != str(move):\n",
        "    #   print(\"found diff\")\n",
        "    #   print(\"action: \", fr, fc, plane)\n",
        "    #   print(\"decoded: \", decode_action((fr, fc, plane)))\n",
        "    #   print(\"Coords: \", fr, fc, tr, tc)\n",
        "    #   print(move)\n",
        "    #   print(str(move) == coord_to_uci(fr, fc, tr, tc))\n",
        "    #   raise Exception(\"found differing in uci\")\n",
        "\n",
        "    return (fr, fc, plane)\n",
        "\n",
        "  def apply(self, action):\n",
        "    uci = decode_action(action)\n",
        "    if action[0] == 1 and len(uci) == 4 and str(self.board.piece_at(action[0]*8+action[1])) == 'p':\n",
        "      uci += 'q'\n",
        "    if action[0] == 6 and len(uci) == 4 and str(self.board.piece_at(action[0]*8+action[1])) == 'P':\n",
        "      uci += 'q'\n",
        "    self.board.push_uci(uci)\n",
        "    self.update_history()\n",
        "    #print(self.board)\n",
        "\n",
        "  def generate_legal_actions(self):\n",
        "    moves = self.generate_legal_moves()\n",
        "    actions = [self.encode_action(move) for move in moves]\n",
        "    return actions\n",
        "\n",
        "  def update_history(self):\n",
        "    fen = self.board.fen()\n",
        "    s_fen = fen.split(' ')\n",
        "    cs_fen = ' '.join(s_fen[:-2])\n",
        "\n",
        "    if cs_fen in self.repetitions:\n",
        "      self.repetitions[cs_fen] += 1\n",
        "      repeats = self.repetitions[cs_fen]\n",
        "    else:\n",
        "      self.repetitions[cs_fen] = 1\n",
        "      repeats = 1\n",
        "\n",
        "    halfmove_clock = self.board.halfmove_clock\n",
        "    self.board_history.append((fen, repeats))\n",
        "\n",
        "  def make_image(self, state_index=-1):\n",
        "    fen, repeats= self.board_history[state_index]\n",
        "\n",
        "    repr = fen_to_repr(fen, repeats)\n",
        "    return repr"
      ],
      "metadata": {
        "id": "3FE-c9peKBP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Game(object):\n",
        "\n",
        "  def __init__(self, history=None, environment=None):\n",
        "    self.history = history or []\n",
        "    self.child_visits = []\n",
        "    self.num_actions = 4672  # action space size for chess\n",
        "    self.environment = Environment(environment)\n",
        "\n",
        "  def terminal(self):\n",
        "    # Game specific termination rules.\n",
        "    return self.environment.is_terminal()\n",
        "\n",
        "  def terminal_value(self, to_play):\n",
        "    # Game specific value.\n",
        "    return self.environment.terminal_value(to_play)\n",
        "\n",
        "  def legal_actions(self):\n",
        "    # Game specific calculation of legal actions.\n",
        "    return self.environment.generate_legal_actions()\n",
        "\n",
        "  def clone(self):\n",
        "    return Game(list(self.history), self.environment)\n",
        "\n",
        "  def apply(self, action):\n",
        "    self.history.append(action)\n",
        "    self.environment.apply(action)\n",
        "\n",
        "  def store_search_statistics(self, root):\n",
        "    action_scores = np.zeros((8, 8, 73))\n",
        "    sum_visits = sum(child.visit_count for child in root.children.values())\n",
        "\n",
        "    for action, child in root.children.items():\n",
        "      action_scores[action] = child.visit_count / sum_visits\n",
        "\n",
        "    self.child_visits.append(action_scores)\n",
        "\n",
        "    # self.child_visits.append([\n",
        "    #     root.children[a].visit_count / sum_visits if a in root.children else 0\n",
        "    #     for a in range(self.num_actions)\n",
        "    # ])\n",
        "\n",
        "  def make_image(self, state_index: int):\n",
        "    # Game specific feature planes.\n",
        "    return self.environment.make_image(state_index)\n",
        "\n",
        "  def make_target(self, state_index: int):\n",
        "    return (self.terminal_value(state_index % 2),\n",
        "            self.child_visits[state_index])\n",
        "\n",
        "  def to_play(self): # 0: White, 1: Black\n",
        "    return len(self.history) % 2\n"
      ],
      "metadata": {
        "id": "rSCKGOM9zjwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, config: Config):\n",
        "    self.window_size = config.window_size\n",
        "    self.batch_size = config.batch_size\n",
        "    self.buffer = []\n",
        "\n",
        "  def save_game(self, game):\n",
        "    if len(self.buffer) > self.window_size:\n",
        "      self.buffer.pop(0)\n",
        "    self.buffer.append(game)\n",
        "\n",
        "  def sample_batch(self):\n",
        "    # Sample uniformly across positions.\n",
        "    #print(f\"Sampling {self.batch_size} from: {len(self.buffer)}\")\n",
        "    move_sum = float(sum(len(g.history) for g in self.buffer))\n",
        "    games = np.random.choice(\n",
        "        self.buffer,\n",
        "        size=self.batch_size,\n",
        "        p=[len(g.history) / move_sum for g in self.buffer])\n",
        "    game_pos = [(g, np.random.randint(len(g.history))) for g in games]\n",
        "    return [(g.make_image(i), g.make_target(i)) for (g, i) in game_pos]\n",
        "\n",
        "  def size(self):\n",
        "    return len(self.buffer)"
      ],
      "metadata": {
        "id": "En06LbK6zoRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(object):\n",
        "\n",
        "  def __init__(self, uniform_model: bool=False):\n",
        "    if uniform_model:\n",
        "      self.model = make_uniform_network()\n",
        "    else:\n",
        "      self.model = make_network()\n",
        "      self.trainable_variables = self.model.trainable_variables\n",
        "\n",
        "  def inference(self, image): # inference for SINGLE IMAGE\n",
        "      # Run the neural network model to get predictions\n",
        "      image = np.expand_dims(image, axis=0)\n",
        "      value, policy_logits = self.model.predict(image, verbose=0)\n",
        "\n",
        "      value = value[0] # The value output is a scalar representing the predicted game outcome\n",
        "      policy_logits = policy_logits[0]\n",
        "\n",
        "      return value, np.array(policy_logits)\n",
        "\n",
        "  def grad_inference(self, image):\n",
        "      # Preprocess the image\n",
        "      image = np.expand_dims(image, axis=0)\n",
        "\n",
        "      # Use the `call` method for gradient-aware operations\n",
        "      with tf.GradientTape() as tape:\n",
        "          value, policy_logits = self.model(image, training=False) #predictable, inference-like manner, even though this operation is part of the larger training process\n",
        "      # Post-process the outputs if necessary\n",
        "      value = value[0]  # The value output is a scalar representing the predicted game outcome\n",
        "      policy_logits = policy_logits[0]\n",
        "\n",
        "      return value, policy_logits\n",
        "\n",
        "\n",
        "  def batch_inference(self, images):\n",
        "    # Run the neural network model to get predictions\n",
        "    values, policy_logits = self.model.predict(images, verbose=0)\n",
        "    return values, np.array(policy_logits)\n",
        "\n",
        "\n",
        "  def batch_grad_inference(self, images):\n",
        "\n",
        "      # Use the `call` method for gradient-aware operations\n",
        "      with tf.GradientTape() as tape:\n",
        "          values, policy_logits = self.model(images, training=False) #predictable, inference-like manner, even though this operation is part of the larger training process\n",
        "\n",
        "      return values, np.array(policy_logits)\n",
        "\n",
        "\n",
        "\n",
        "  def get_weights(self):\n",
        "    # Returns the weights of this network.\n",
        "    return self.model.get_weights()"
      ],
      "metadata": {
        "id": "HPr_d9UOzqBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir networks\n",
        "class SharedStorage(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._networks = {}\n",
        "    self.network_count = 0\n",
        "\n",
        "  def latest_network(self) -> Network:\n",
        "    if self._networks:\n",
        "      return self._networks[max(self._networks.keys())]\n",
        "    else:\n",
        "      return None  # policy -> uniform, value -> 0.5\n",
        "\n",
        "  def save_network(self, step: int, network: Network):\n",
        "    with open(f\"drive/MyDrive/chessnn_networks/network_0th_{self.network_count}.nn'\", \"wb\") as nn:\n",
        "      pickle.dump(network, nn)\n",
        "\n",
        "    self._networks[step] = network\n",
        "    self.network_count += 1\n"
      ],
      "metadata": {
        "id": "6UsTnTQKzreQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training is split into two independent parts: Network training and\n",
        "# self-play data generation.\n",
        "# These two parts only communicate by transferring the latest network checkpoint\n",
        "# from the training to the self-play, and the finished games from the self-play\n",
        "# to the training.\n",
        "def zero(config: Config):\n",
        "  storage = SharedStorage()\n",
        "  replay_buffer = ReplayBuffer(config)\n",
        "  run_selfplay(config, storage, replay_buffer)\n",
        "  threads = [Thread(target=run_selfplay, args=(config, storage, replay_buffer))\n",
        "            for _ in range(config.num_actors)]\n",
        "\n",
        "  for t in threads:\n",
        "    t.start()\n",
        "\n",
        "  print(\"Self-play data generation launched.\")\n",
        "\n",
        "  train_network(config, storage, replay_buffer)\n",
        "\n",
        "  return storage.latest_network()\n",
        "\n"
      ],
      "metadata": {
        "id": "tV9QkBmLBKT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "####### Part 1: Self-Play ########\n",
        "\n",
        "\n",
        "# Each self-play job is independent of all others; it takes the latest network\n",
        "# snapshot, produces a game and makes it available to the training job by\n",
        "# writing it to a shared replay buffer.\n",
        "def run_selfplay(config: Config, storage: SharedStorage,\n",
        "                 replay_buffer: ReplayBuffer):\n",
        "  while True:\n",
        "    network = storage.latest_network()\n",
        "    game = play_game(config, network)\n",
        "    replay_buffer.save_game(game)\n",
        "\n",
        "\n",
        "# Each game is produced by starting at the initial board position, then\n",
        "# repeatedly executing a Monte Carlo Tree Search to generate moves until the end\n",
        "# of the game is reached.\n",
        "def play_game(config: Config, network: Network=None):\n",
        "  num_simulations = config.num_simulations\n",
        "  num_sampling_moves = config.num_sampling_moves\n",
        "  softmax_temperature = config.softmax_temperature\n",
        "  if not network:\n",
        "    network = Network(True)\n",
        "    num_simulations = config.uniform_num_simulations\n",
        "    num_sampling_moves = config.uniform_num_sampling_moves\n",
        "    softmax_temperature = config.uniform_softmax_temperature\n",
        "\n",
        "  game = Game()\n",
        "  while not game.terminal() and len(game.history) < config.max_moves:\n",
        "    action, root = run_mcts(config, game, network, num_simulations, num_sampling_moves, softmax_temperature)\n",
        "    game.apply(action)\n",
        "    game.store_search_statistics(root)\n",
        "  return game\n",
        "\n",
        "\n",
        "# Core Monte Carlo Tree Search algorithm.\n",
        "# To decide on an action, we run N simulations, always starting at the root of\n",
        "# the search tree and traversing the tree according to the UCB formula until we\n",
        "# reach a leaf node.\n",
        "def run_mcts(config: Config, game: Game, network: Network, num_simulations: int=800, num_sampling_moves=30, softmax_temperature=10):\n",
        "  root = Node(0)\n",
        "  evaluate(root, game, network)\n",
        "  add_exploration_noise(config, root)\n",
        "\n",
        "  for _ in range(num_simulations):\n",
        "    node = root\n",
        "    scratch_game = game.clone()\n",
        "    search_path = [node]\n",
        "\n",
        "    while node.expanded():\n",
        "      action, node = select_child(config, node)\n",
        "      scratch_game.apply(action)\n",
        "      search_path.append(node)\n",
        "\n",
        "    value = evaluate(node, scratch_game, network)\n",
        "    backpropagate(search_path, value, scratch_game.to_play())\n",
        "\n",
        "  return select_action(config, game, root, num_sampling_moves, softmax_temperature), root\n",
        "\n",
        "\n",
        "\n",
        "def softmax_sample(visit_counts, temperature=10.0):\n",
        "  counts, actions = zip(*visit_counts)\n",
        "\n",
        "  if temperature == -1: # pure random\n",
        "    rnd_idx = np.random.choice(len(actions))\n",
        "    return None, actions[rnd_idx]\n",
        "\n",
        "  # Apply softmax with temperature\n",
        "  counts = np.array(counts)\n",
        "  counts = counts / temperature  # Apply temperature scaling\n",
        "  softmax_probs = np.exp(counts) / sum(np.exp(counts))\n",
        "  # Sample an action based on the softmax probabilities\n",
        "  rnd_idx = np.random.choice(len(actions), p=softmax_probs)\n",
        "  action = actions[rnd_idx]\n",
        "  return softmax_probs, action\n",
        "\n",
        "\n",
        "def select_action(config: Config, game: Game, root: Node, num_sampling_moves=30, softmax_temperature=10):\n",
        "  visit_counts = [(child.visit_count, action)\n",
        "                  for action, child in root.children.items()]\n",
        "  if len(game.history) < num_sampling_moves:\n",
        "    _, action = softmax_sample(visit_counts, softmax_temperature)\n",
        "  else:\n",
        "    _, action = max(visit_counts)\n",
        "  return action\n",
        "\n",
        "\n",
        "# Select the child with the highest UCB score.\n",
        "def select_child(config: Config, node: Node):\n",
        "  pb_c = math.log((node.visit_count + config.pb_c_base + 1) /\n",
        "                config.pb_c_base) + config.pb_c_init\n",
        "  _, action, child = max((ucb_score(config, node, child, pb_c), action, child)\n",
        "                         for action, child in node.children.items())\n",
        "  return action, child\n",
        "\n",
        "\n",
        "# The score for a node is based on its value, plus an exploration bonus based on\n",
        "# the prior.\n",
        "def ucb_score(config: Config, parent: Node, child: Node, pb_c):\n",
        "  pb_C = pb_c * math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "  prior_score = pb_C * child.prior\n",
        "  value_score = child.value()\n",
        "  return prior_score + value_score\n",
        "\n",
        "\n",
        "# We use the neural network to obtain a value and policy prediction.\n",
        "def evaluate(node: Node, game: Game, network: Network):\n",
        "  value, policy_logits = network.inference(game.make_image(-1))\n",
        "\n",
        "  # Expand the node.\n",
        "  node.to_play = game.to_play()\n",
        "\n",
        "  policy = {a: math.exp(policy_logits[a]) for a in game.legal_actions()}\n",
        "  policy_sum = sum(policy.values())\n",
        "  for action, p in policy.items():\n",
        "    node.children[action] = Node(p / policy_sum)\n",
        "  return value\n",
        "\n",
        "\n",
        "# At the end of a simulation, we propagate the evaluation all the way up the\n",
        "# tree to the root.\n",
        "def backpropagate(search_path: List[Node], value: float, to_play):\n",
        "  for node in search_path:\n",
        "    node.value_sum += value if node.to_play == to_play else (1 - value)\n",
        "    node.visit_count += 1\n",
        "\n",
        "\n",
        "# At the start of each search, we add dirichlet noise to the prior of the root\n",
        "# to encourage the search to explore new actions.\n",
        "def add_exploration_noise(config: Config, node: Node):\n",
        "  actions = node.children.keys()\n",
        "  noise = np.random.gamma(config.root_dirichlet_alpha, 1, len(actions))\n",
        "  frac = config.root_exploration_fraction\n",
        "  for a, n in zip(actions, noise):\n",
        "    node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "######### End Self-Play ##########\n",
        "##################################\n",
        "\n",
        "##################################\n",
        "####### Part 2: Training #########\n",
        "\n",
        "class ZeroLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, lr_schedule):\n",
        "        super(ZeroLearningRateSchedule, self).__init__()\n",
        "        # Convert keys to integers\n",
        "        self.lr_schedule = {int(k): v for k, v in lr_schedule.items()}\n",
        "        self.lr_schedule_keys = sorted(self.lr_schedule)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        learning_rate = self.lr_schedule[self.lr_schedule_keys[0]]\n",
        "        for i in range(1, len(self.lr_schedule_keys)):\n",
        "            if step < self.lr_schedule_keys[i]:\n",
        "                break\n",
        "            learning_rate = self.lr_schedule[self.lr_schedule_keys[i]]\n",
        "        return learning_rate\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'lr_schedule': self.lr_schedule}\n",
        "\n",
        "\n",
        "def train_network(config: Config, storage: SharedStorage,\n",
        "                  replay_buffer: ReplayBuffer):\n",
        "  network = Network()\n",
        "  learning_rate_schedule = ZeroLearningRateSchedule(config.learning_rate_schedule)\n",
        "  optimizer = tf.keras.optimizers.SGD(\n",
        "      learning_rate_schedule,\n",
        "      config.momentum,\n",
        "      nesterov=True # testin nestrov\n",
        "      )\n",
        "  print(f\"Training network with {config.training_steps} steps and batch size {config.batch_size}.\")\n",
        "  print(f\"Optimizer configuration: \")\n",
        "  print(optimizer.get_config())\n",
        "\n",
        "\n",
        "\n",
        "  for i in range(config.training_steps):\n",
        "    batch = replay_buffer.sample_batch()\n",
        "    update_weights(optimizer, network, batch, config.weight_decay)\n",
        "    if i % config.checkpoint_interval == config.checkpoint_interval-1:\n",
        "      print(f\"Checkpoint at training step: {i}\")\n",
        "      storage.save_network(i, network)\n",
        "      print(\"Current Learning Rate:\",optimizer.learning_rate.numpy())\n",
        "\n",
        "  storage.save_network(config.training_steps, network)\n",
        "\n",
        "\n",
        "def update_weights(optimizer: tf.keras.optimizers, network: Network, batch, weight_decay: float):\n",
        "    images, targets = zip(*batch)\n",
        "    target_values, target_policies = zip(*targets)\n",
        "\n",
        "    images = np.stack(images)\n",
        "    target_values = np.stack(target_values)\n",
        "    target_policies = np.stack(target_policies)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        values, policy_logits = network.batch_grad_inference(images)\n",
        "\n",
        "        reshaped_target_policies = tf.reshape(target_policies, [-1, 4672])  # Reshape to match logits\n",
        "        reshaped_policy_logits = tf.reshape(policy_logits, [-1, 4672])\n",
        "\n",
        "        # Calculate the losses for the entire batch\n",
        "        value_loss = tf.losses.mean_squared_error(target_values, values)\n",
        "        policy_loss = tf.nn.softmax_cross_entropy_with_logits(labels=reshaped_target_policies, logits=reshaped_policy_logits)\n",
        "        loss = tf.reduce_mean(value_loss + policy_loss)\n",
        "\n",
        "        # Regularization (if applicable)\n",
        "        for weights in network.trainable_variables:\n",
        "            loss += weight_decay * tf.nn.l2_loss(weights)\n",
        "\n",
        "\n",
        "        gradients = tape.gradient(loss, network.trainable_variables)\n",
        "\n",
        "        if any(tf.math.is_nan(tf.norm(grad)).numpy() for grad in gradients if grad is not None):\n",
        "          print(\"Exploding gradients detected.\")\n",
        "\n",
        "    # Gradient clipping\n",
        "    if config.clipnorm is not None:\n",
        "        gradients, _ = tf.clip_by_global_norm(gradients, config.clipnorm)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, network.trainable_variables))\n",
        "    print(f\"Loss: {loss}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######### End Training ###########\n",
        "##################################\n",
        "\n",
        "\n",
        "def launch_job(f, *args):\n",
        "  f(*args)"
      ],
      "metadata": {
        "id": "Ou5wkaIzpvvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir tmp\n",
        "!mkdir gifs\n",
        "!mkdir uci_pgn\n",
        "\n",
        "def svgs_to_pngs(svgs_data):\n",
        "  for i in range(len(svgs_data)):\n",
        "    cairosvg.svg2png(bytestring=svgs_data[i], write_to=f\"tmp/board_img_{i}.png\")\n",
        "\n",
        "\n",
        "def svgs_to_gif(svgs, game_idx):\n",
        "  svgs_to_pngs(svgs)\n",
        "\n",
        "  images = []\n",
        "  for i in range(len(svgs)):  # Assumes all PNGs are in the current directory\n",
        "      images.append(Image.open(f\"tmp/board_img_{i}.png\"))\n",
        "\n",
        "  images[0].save(f'gifs/game_{game_idx}_moves_{len(svgs)}.gif', save_all=True, append_images=images[1:], duration=200, loop=0)\n",
        "\n",
        "  return f'gifs/game_{game_idx}_moves_{len(svgs)}.gif'\n",
        "\n",
        "\n",
        "def display_svg(board: chess.Board, svgs, show_svg: bool=False):\n",
        "  boardsvg = chess.svg.board(board, size=350)\n",
        "  svgs.append(boardsvg)\n",
        "  if show_svg:\n",
        "    display(IPython.display.HTML(boardsvg))\n",
        "\n",
        "def visualize_game(config: Config, network: Network=None, show_svg: bool=True, store_gif: bool=False, game_idx: int=0):\n",
        "  num_simulations = config.num_simulations\n",
        "  num_sampling_moves = config.num_sampling_moves\n",
        "  softmax_temperature = config.softmax_temperature\n",
        "  if not network:\n",
        "    network = Network(True)\n",
        "    num_simulations = config.uniform_num_simulations\n",
        "    num_sampling_moves = config.uniform_num_sampling_moves\n",
        "    softmax_temperature = config.uniform_softmax_temperature\n",
        "\n",
        "  game = Game()\n",
        "  svgs = []\n",
        "\n",
        "  # initial board\n",
        "  display_svg(game.environment.board, svgs, show_svg)\n",
        "\n",
        "  while not game.terminal() and len(game.history) < config.max_moves:\n",
        "    action, root = run_mcts(config, game, network, num_simulations, num_sampling_moves, softmax_temperature)\n",
        "    game.apply(action)\n",
        "    display_svg(game.environment.board, svgs, show_svg)\n",
        "    game.store_search_statistics(root)\n",
        "\n",
        "  print(\"Game Ended with ply: \", game.environment.board.ply())\n",
        "\n",
        "  uci_pgn = \"\"\n",
        "  count = 2\n",
        "  for move in game.environment.board.move_stack:\n",
        "    if count % 10 == 0:\n",
        "      uci_pgn += \"\\n\"\n",
        "    if count % 2 == 0:\n",
        "      uci_pgn += f\"{count//2}.\"\n",
        "    uci_pgn += move.uci() + \" \"\n",
        "    count += 1\n",
        "\n",
        "  with open(f\"uci_pgn/game_{game_idx}_moves_{len(svgs)}.txt\", \"w\") as text_file:\n",
        "      text_file.write(uci_pgn)\n",
        "\n",
        "  if store_gif:\n",
        "    file_name = svgs_to_gif(svgs, game_idx)\n",
        "    print(\"Gif created at: \", file_name)\n",
        "\n",
        "  return game"
      ],
      "metadata": {
        "id": "H6Fhx_64I52B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir games\n",
        "\n",
        "def dump_game(game, name):\n",
        "  with open(f'games/{name}.game', 'wb') as game_file:\n",
        "    pickle.dump(game, game_file)\n",
        "  return f'games/{name}.game'"
      ],
      "metadata": {
        "id": "892RJjOme9ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_visualize_game(config:Config=None, network:Network=None, SVG=True, GIF=True):\n",
        "  if not config:\n",
        "    config = Config()\n",
        "\n",
        "  for i in range(1000):\n",
        "    game = visualize_game(config, network=network, show_svg=SVG, store_gif=GIF, game_idx=i)\n",
        "    name = dump_game(game, f\"uniform_{i}\")\n",
        "    print(\"Game stored at: \", name)"
      ],
      "metadata": {
        "id": "jIc4rw9TLZyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_visualize_game()"
      ],
      "metadata": {
        "id": "wEM8hvBWRhWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pklbuf\n",
        "\n",
        "class PickledBuffer(object):\n",
        "\n",
        "  def __init__(self, name: str = \"pickled_buffer\", max_buffer_size:int=128):\n",
        "    self.max_buffer_size = max_buffer_size\n",
        "    self.idx = 0\n",
        "    self.name = name\n",
        "    self.buffer = []\n",
        "\n",
        "  def offload(self):\n",
        "    print(f\"Offloading pickled buffer '{self.name}' with {len(self.buffer)} games at 'pklbuf/{self.name}_{self.idx}.pb'.\")\n",
        "    if self.buffer:\n",
        "      with open(f'pklbuf/{self.name}_{self.idx}.pb', 'wb') as pickled_buffer:\n",
        "        pickle.dump(self.buffer, pickled_buffer)\n",
        "      self.buffer.clear()\n",
        "      files.download(f'pklbuf/{self.name}_{self.idx}.pb')\n",
        "      self.idx += 1\n",
        "\n",
        "  def save_game(self, game):\n",
        "    if len(self.buffer) >= self.max_buffer_size:\n",
        "      self.offload()\n",
        "    self.buffer.append(game)\n",
        "\n",
        "  def size(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def pickles(self):\n",
        "    return self.idx\n",
        "\n",
        "\n",
        "\n",
        "def merge_pickled_buffers(config: Config, pickles:int = None, name:str =\"pickled_buffer\") -> ReplayBuffer:\n",
        "  replay_buffer = ReplayBuffer(config)\n",
        "  if pickles:\n",
        "    for i in range(pickles):\n",
        "      with open(f'pklbuf/{name}_{i}.pb', 'rb') as pickled_buffer:\n",
        "        pickled_buffer = pickle.load(pickled_buffer)\n",
        "      replay_buffer.buffer += pickled_buffer\n",
        "  else:\n",
        "    idx = 0\n",
        "    while True:\n",
        "      try:\n",
        "        with open(f'pklbuf/{name}_{idx}.pb', 'rb') as pickled_buffer:\n",
        "          pickled_buffer = pickle.load(pickled_buffer)\n",
        "          replay_buffer.extend(pickled_buffer)\n",
        "          idx += 1\n",
        "      except:\n",
        "        break\n",
        "  return replay_buffer"
      ],
      "metadata": {
        "id": "YPWXb4kzMxYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir buffer\n",
        "def fill_buffer(replay_buffer, config: Config = None, storage: SharedStorage=None, num_games: int=None):\n",
        "  if not config:\n",
        "    config = Config()\n",
        "  if not storage:\n",
        "    storage = SharedStorage()\n",
        "\n",
        "  if not num_games:\n",
        "    num_games = config.window_size\n",
        "\n",
        "  for i in range(num_games):\n",
        "\n",
        "    network = storage.latest_network()\n",
        "    game = play_game(config, network)\n",
        "    replay_buffer.save_game(game)\n",
        "    if type(replay_buffer) == ReplayBuffer and i % 128 == 127:\n",
        "      with open(f'buffer/uniform_step_0_sim.buffer', 'wb') as buffer:\n",
        "        pickle.dump(replay_buffer, buffer)\n",
        "      print(\"Checkpoint: \", i // 128)"
      ],
      "metadata": {
        "id": "uqqVc_alhnOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7vdv3pGM4XY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "\n",
        "config.training_steps=10\n",
        "#config.clipnorm = 0.08\n",
        "config.clipnorm = 0.001\n",
        "#config.learning_rate_schedule[0] = 1e-5\n",
        "\n",
        "num_simulations = 32\n",
        "num_sampling_moves = 30\n",
        "softmax_temperature = 10\n",
        "\n",
        "\n",
        "\n",
        "storage = SharedStorage()\n",
        "\n",
        "game_idx = 0\n",
        "\n",
        "while True:\n",
        "  replay_buffer = ReplayBuffer(config)\n",
        "  network = storage.latest_network()\n",
        "\n",
        "  if not network:\n",
        "    network = Network(True)\n",
        "\n",
        "  for i in range(32):\n",
        "    game = Game()\n",
        "    svgs = []\n",
        "\n",
        "    # initial board\n",
        "    if i % 32 == 31:\n",
        "      display_svg(game.environment.board, svgs, False)\n",
        "\n",
        "    while not game.terminal() and len(game.history) < config.max_moves:\n",
        "      action, root = run_mcts(config, game, network, num_simulations, num_sampling_moves, softmax_temperature)\n",
        "      game.apply(action)\n",
        "      if i % 32 == 31:\n",
        "        display_svg(game.environment.board, svgs, False)\n",
        "      game.store_search_statistics(root)\n",
        "\n",
        "    print(f\"Ply: {len(game.history)}\", end=\", \")\n",
        "\n",
        "    if i % 32 == 31:\n",
        "      uci_pgn = \"\"\n",
        "      count = 2\n",
        "      for move in game.environment.board.move_stack:\n",
        "        if count % 10 == 0:\n",
        "          uci_pgn += \"\\n\"\n",
        "        if count % 2 == 0:\n",
        "          uci_pgn += f\"{count//2}.\"\n",
        "        uci_pgn += move.uci() + \" \"\n",
        "        count += 1\n",
        "\n",
        "      with open(f\"uci_pgn/game_{game_idx}_moves_{len(svgs)}.txt\", \"w\") as text_file:\n",
        "          text_file.write(uci_pgn)\n",
        "\n",
        "    if i % 32 == 31:\n",
        "      print()\n",
        "      file_name = svgs_to_gif(svgs, game_idx)\n",
        "      game_idx += 1\n",
        "      print(\"Gif created at: \", file_name)\n",
        "\n",
        "    replay_buffer.save_game(game)\n",
        "\n",
        "  train_network(config, storage, replay_buffer)\n"
      ],
      "metadata": {
        "id": "YR5gtJ9zhgHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.training_steps=1\n",
        "train_network(config, storage, replay_buffer)"
      ],
      "metadata": {
        "id": "J1OnVxe21Utt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "config.num_simulations=10\n",
        "pickled_buffer = PickledBuffer(name=\"pickled_buffer\", max_buffer_size=256)\n",
        "storage = SharedStorage()\n",
        "config.uniform_num_simulations=1\n",
        "fill_buffer(config=config, replay_buffer=pickled_buffer, storage=storage, num_games=int(1e5))"
      ],
      "metadata": {
        "id": "rm2Tw2jAFwKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(f'buffer/uniform_step_0_sim.buffer', 'rb') as buffer:\n",
        "#   replay_buffer = pickle.load(buffer)"
      ],
      "metadata": {
        "id": "RwdvPZ-WIV9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickled_buffer.offload() # clear buffer\n",
        "pickles = pickled_buffer.pickles()\n",
        "\n",
        "replay_buffer = merge_pickled_buffers(config, pickles=pickles)\n",
        "print(replay_buffer.size())"
      ],
      "metadata": {
        "id": "37t5uJ7QQcfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "replay_buffer.batch_size=32\n",
        "config = Config()\n",
        "config.training_steps=10\n",
        "\n",
        "#config.clipnorm = 0.08\n",
        "config.clipnorm = 0.001\n",
        "#config.learning_rate_schedule[0] = 1e-5\n",
        "storage = SharedStorage()\n",
        "\n",
        "train_network(config, storage, replay_buffer)"
      ],
      "metadata": {
        "id": "yKzGA77G6h2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "network = storage.latest_network()\n",
        "test_visualize_game(config, network, SVG=True, GIF=True)"
      ],
      "metadata": {
        "id": "j4fsWWy8QBY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "game = Game()\n",
        "image = game.make_image()"
      ],
      "metadata": {
        "id": "UuEsM6QKQore"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}