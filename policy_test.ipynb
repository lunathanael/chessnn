{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMydO7cRjbNcrKbLjqdb/QD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lunathanael/chessnn/blob/main/policy_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "aEOVZHf_fRtr"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import math\n",
        "import typing\n",
        "from typing import Dict, List, Optional\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "from typing import List\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Add, Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAXIMUM_FLOAT_VALUE = float('inf')\n",
        "\n",
        "KnownBounds = collections.namedtuple('KnownBounds', ['min', 'max'])"
      ],
      "metadata": {
        "id": "RIE-oRMbDOlS"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MinMaxStats(object):\n",
        "  \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
        "\n",
        "  def __init__(self, known_bounds: Optional[KnownBounds]):\n",
        "    self.maximum = known_bounds.max if known_bounds else -MAXIMUM_FLOAT_VALUE\n",
        "    self.minimum = known_bounds.min if known_bounds else MAXIMUM_FLOAT_VALUE\n",
        "\n",
        "  def update(self, value: float):\n",
        "    self.maximum = max(self.maximum, value)\n",
        "    self.minimum = min(self.minimum, value)\n",
        "\n",
        "  def normalize(self, value: float) -> float:\n",
        "    if self.maximum > self.minimum:\n",
        "      # We normalize only when we have set the maximum and minimum values.\n",
        "      return (value - self.minimum) / (self.maximum - self.minimum)\n",
        "    return value"
      ],
      "metadata": {
        "id": "6ZCEDG5ADcEx"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "\n",
        "\n",
        "  def __init__(self,\n",
        "               action_space_size: int,\n",
        "               max_moves: int,\n",
        "               discount: float,\n",
        "               dirichlet_alpha: float,\n",
        "               num_simulations: int,\n",
        "               batch_size: int,\n",
        "               td_steps: int,\n",
        "               num_actors: int,\n",
        "               lr_init: float,\n",
        "               lr_decay_steps: float,\n",
        "               visit_softmax_temperature_fn,\n",
        "               known_bounds: Optional[KnownBounds] = None):\n",
        "    ### Self-Play\n",
        "    self.action_space_size = action_space_size\n",
        "    self.num_actors = num_actors\n",
        "\n",
        "    self.visit_softmax_temperature_fn = visit_softmax_temperature_fn\n",
        "    self.max_moves = max_moves\n",
        "    self.num_simulations = num_simulations\n",
        "    self.discount = discount\n",
        "\n",
        "    # Root prior exploration noise.\n",
        "    self.root_dirichlet_alpha = dirichlet_alpha\n",
        "    self.root_exploration_fraction = 0.25\n",
        "\n",
        "    # UCB formula\n",
        "    self.pb_c_base = 19652\n",
        "    self.pb_c_init = 1.25\n",
        "\n",
        "    # If we already have some information about which values occur in the\n",
        "    # environment, we can use them to initialize the rescaling.\n",
        "    # This is not strictly necessary, but establishes identical behaviour to\n",
        "    # AlphaZero in board games.\n",
        "    self.known_bounds = known_bounds\n",
        "\n",
        "    ### Training\n",
        "    self.training_steps = int(1000e3)\n",
        "    self.checkpoint_interval = int(1e3)\n",
        "    self.window_size = int(1e6)\n",
        "    self.batch_size = batch_size\n",
        "    self.num_unroll_steps = 5\n",
        "    self.td_steps = td_steps\n",
        "\n",
        "    self.weight_decay = 1e-4\n",
        "    self.momentum = 0.9\n",
        "\n",
        "    # Exponential learning rate schedule\n",
        "    self.lr_init = lr_init\n",
        "    self.lr_decay_rate = 0.1\n",
        "    self.lr_decay_steps = lr_decay_steps\n",
        "\n",
        "  def new_game(self):\n",
        "    return Game(self.action_space_size, self.discount)"
      ],
      "metadata": {
        "id": "EFGx4mQURtmE"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChessNetwork(tf.keras.Model):\n",
        "    def __init__(self, config: Config, num_res_blocks=16, num_filters=256):\n",
        "        super(ChessNetwork, self).__init__()\n",
        "        self.action_size = config.action_space_size\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.num_filters = num_filters\n",
        "\n",
        "        # Representation Function\n",
        "        self.representation_conv = tf.keras.layers.Conv2D(num_filters, 3, padding='same', activation='relu')\n",
        "        self.representation_res_blocks = [self._build_residual_block() for _ in range(num_res_blocks)]\n",
        "\n",
        "        # Dynamics Function\n",
        "        self.dynamics_action_conv = tf.keras.layers.Conv2D(num_filters, 3, padding='same', activation='relu')\n",
        "        self.dynamics_res_blocks = [self._build_residual_block() for _ in range(num_res_blocks)]\n",
        "        self.dynamics_reward = tf.keras.layers.Dense(1, activation='tanh')\n",
        "\n",
        "        # Prediction Function\n",
        "        self.prediction_policy = tf.keras.layers.Dense(self.action_size)  # policy logits\n",
        "        self.prediction_value = tf.keras.layers.Dense(1, activation='tanh')  # value\n",
        "\n",
        "    def _build_residual_block(self):\n",
        "        return tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv2D(self.num_filters, 3, padding='same', activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Conv2D(self.num_filters, 3, padding='same', activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization()\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs, action=None):\n",
        "      # Process inputs through the representation function\n",
        "      if action is None:\n",
        "        hidden_state = self.representation_conv(inputs)\n",
        "        for block in self.representation_res_blocks:\n",
        "            hidden_state = block(hidden_state)\n",
        "        reward = tf.constant(0.0)\n",
        "      else:\n",
        "        hidden_state = tf.concat([inputs, action], axis=-1)\n",
        "        hidden_state = self.dynamics_action_conv(hidden_state)\n",
        "        for block in self.dynamics_res_blocks:\n",
        "            hidden_state = block(hidden_state)\n",
        "\n",
        "      reward = self.dynamics_reward(hidden_state)\n",
        "      policy_logits = self.prediction_policy(hidden_state)\n",
        "      value = self.prediction_value(hidden_state)\n",
        "\n",
        "      return NetworkOutput(value, reward, policy_logits, hidden_state)\n",
        "\n",
        "\n",
        "class UniformChessNetwork(tf.keras.Model):\n",
        "    def __init__(self, config: Config, num_res_blocks=16, num_filters=256):\n",
        "        super(UniformChessNetwork, self).__init__()\n",
        "        self.action_size = config.action_space_size\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.num_filters = num_filters\n",
        "\n",
        "        # Representation Function\n",
        "        self.representation_conv = tf.keras.layers.Conv2D(num_filters, 3, padding='same', activation='relu')\n",
        "        self.representation_res_blocks = [self._build_residual_block() for _ in range(num_res_blocks)]\n",
        "\n",
        "        # Dynamics Function\n",
        "        self.dynamics_action_conv = tf.keras.layers.Conv2D(num_filters, 3, padding='same', activation='relu')\n",
        "        self.dynamics_res_blocks = [self._build_residual_block() for _ in range(num_res_blocks)]\n",
        "\n",
        "    def _build_residual_block(self):\n",
        "        return tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv2D(self.num_filters, 3, padding='same', activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Conv2D(self.num_filters, 3, padding='same', activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization()\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Implement call method to process inputs through representation and dynamics functions\n",
        "        # Process inputs through the representation function\n",
        "        hidden_state = self.representation_conv(inputs)\n",
        "        for block in self.representation_res_blocks:\n",
        "            hidden_state = block(hidden_state)\n",
        "\n",
        "        # Uniform policy logits and fixed values for reward and value\n",
        "        uniform_policy_logits = tf.fill([self.action_size], tf.math.log(1.0 / self.action_size))\n",
        "        value = tf.constant(0.0)\n",
        "        reward = tf.constant(0.0)\n",
        "\n",
        "        return NetworkOutput(value, reward, uniform_policy_logits, hidden_state)\n",
        "\n"
      ],
      "metadata": {
        "id": "ABgGpmx0fWGi"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_board_game_config(action_space_size: int, max_moves: int,\n",
        "                           dirichlet_alpha: float,\n",
        "                           lr_init: float) -> Config:\n",
        "\n",
        "  def visit_softmax_temperature(num_moves, training_steps):\n",
        "    if num_moves < 30:\n",
        "      return 1.0\n",
        "    else:\n",
        "      return 0.0  # Play according to the max.\n",
        "\n",
        "  return Config(\n",
        "      action_space_size=action_space_size,\n",
        "      max_moves=max_moves,\n",
        "      discount=1.0,\n",
        "      dirichlet_alpha=dirichlet_alpha,\n",
        "      num_simulations=800,\n",
        "      batch_size=2048,\n",
        "      td_steps=max_moves,  # Always use Monte Carlo return.\n",
        "      num_actors=3000,\n",
        "      lr_init=lr_init,\n",
        "      lr_decay_steps=400e3,\n",
        "      visit_softmax_temperature_fn=visit_softmax_temperature,\n",
        "      known_bounds=KnownBounds(-1, 1))"
      ],
      "metadata": {
        "id": "Q30EAjHEDq1w"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_chess_config() -> Config:\n",
        "  return make_board_game_config(\n",
        "      action_space_size=4672, max_moves=512, dirichlet_alpha=0.3, lr_init=0.1)"
      ],
      "metadata": {
        "id": "FAIovKBJDxnk"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Action(object):\n",
        "\n",
        "  def __init__(self, index: int):\n",
        "    self.index = index\n",
        "\n",
        "  def __hash__(self):\n",
        "    return self.index\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    return self.index == other.index\n",
        "\n",
        "  def __gt__(self, other):\n",
        "    return self.index > other.index\n",
        "\n",
        "class Player(object):\n",
        "  pass"
      ],
      "metadata": {
        "id": "LlZ53-pxD0Co"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helpers"
      ],
      "metadata": {
        "id": "P3mUyAMlzxsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node(object):\n",
        "\n",
        "  def __init__(self, prior: float):\n",
        "    self.visit_count = 0\n",
        "    self.to_play = -1\n",
        "    self.prior = prior\n",
        "    self.value_sum = 0\n",
        "    self.children = {}\n",
        "    self.hidden_state = None\n",
        "    self.reward = 0\n",
        "\n",
        "  def expanded(self) -> bool:\n",
        "    return len(self.children) > 0\n",
        "\n",
        "  def value(self) -> float:\n",
        "    if self.visit_count == 0:\n",
        "      return 0\n",
        "    return self.value_sum / self.visit_count"
      ],
      "metadata": {
        "id": "Lvmt70d1y8HR"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionHistory(object):\n",
        "  \"\"\"Simple history container used inside the search.\n",
        "\n",
        "  Only used to keep track of the actions executed.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, history: List[Action], action_space_size: int):\n",
        "    self.history = list(history)\n",
        "    self.action_space_size = action_space_size\n",
        "\n",
        "  def clone(self):\n",
        "    return ActionHistory(self.history, self.action_space_size)\n",
        "\n",
        "  def add_action(self, action: Action):\n",
        "    self.history.append(action)\n",
        "\n",
        "  def last_action(self) -> Action:\n",
        "    return self.history[-1]\n",
        "\n",
        "  def action_space(self) -> List[Action]:\n",
        "    return [Action(i) for i in range(self.action_space_size)]\n",
        "\n",
        "  def to_play(self):\n",
        "    return len(self.history) % 2"
      ],
      "metadata": {
        "id": "BAGB7rRdEHI1"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment(object):\n",
        "  \"\"\"The environment that interacting with.\"\"\"\n",
        "\n",
        "  def step(self, action):\n",
        "    pass\n",
        "\n",
        "  def is_terminal(self):\n",
        "    pass\n",
        "\n",
        "  def get_legal_actions(self):\n",
        "    pass\n",
        "\n",
        "  def get_image(self, state_index: int):\n",
        "    pass"
      ],
      "metadata": {
        "id": "dPi2-nllEJu_"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Game(object):\n",
        "  \"\"\"A single episode of interaction with the environment.\"\"\"\n",
        "\n",
        "  def __init__(self, action_space_size: int, discount: float):\n",
        "    self.environment = Environment()  # Game specific environment.\n",
        "    self.history = []\n",
        "    self.rewards = []\n",
        "    self.child_visits = []\n",
        "    self.root_values = []\n",
        "    self.action_space_size = action_space_size\n",
        "    self.discount = discount\n",
        "\n",
        "  def terminal(self) -> bool:\n",
        "    # Game specific termination rules.\n",
        "    return self.environment.is_terminal()\n",
        "\n",
        "  def legal_actions(self) -> List[Action]:\n",
        "    # Game specific calculation of legal actions.\n",
        "    return self.environment.get_legal_actions()\n",
        "\n",
        "  def apply(self, action: Action):\n",
        "    reward = self.environment.step(action)\n",
        "    self.rewards.append(reward)\n",
        "    self.history.append(action)\n",
        "\n",
        "  def store_search_statistics(self, root: Node):\n",
        "    sum_visits = sum(child.visit_count for child in root.children.values())\n",
        "    action_space = (Action(index) for index in range(self.action_space_size))\n",
        "    self.child_visits.append([\n",
        "        root.children[a].visit_count / sum_visits if a in root.children else 0\n",
        "        for a in action_space\n",
        "    ])\n",
        "    self.root_values.append(root.value())\n",
        "\n",
        "  def make_image(self, state_index: int):\n",
        "    # Game specific feature planes.\n",
        "    return enviroment.get_image(state_index)\n",
        "\n",
        "  def make_target(self, state_index: int, num_unroll_steps: int, td_steps: int,\n",
        "                  to_play):\n",
        "    # The value target is the discounted root value of the search tree N steps\n",
        "    # into the future, plus the discounted sum of all rewards until then.\n",
        "    targets = []\n",
        "    for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
        "      bootstrap_index = current_index + td_steps\n",
        "      if bootstrap_index < len(self.root_values):\n",
        "        value = self.root_values[bootstrap_index] * self.discount**td_steps\n",
        "      else:\n",
        "        value = 0\n",
        "\n",
        "      for i, reward in enumerate(self.rewards[current_index:bootstrap_index]):\n",
        "        value += reward * self.discount**i  # pytype: disable=unsupported-operands\n",
        "\n",
        "      if current_index < len(self.root_values):\n",
        "        targets.append((value, self.rewards[current_index],\n",
        "                        self.child_visits[current_index]))\n",
        "      else:\n",
        "        # States past the end of games are treated as absorbing states.\n",
        "        targets.append((0, 0, []))\n",
        "    return targets\n",
        "\n",
        "  def to_play(self):\n",
        "    return len(self.history) % 2\n",
        "\n",
        "  def action_history(self) -> ActionHistory:\n",
        "    return ActionHistory(self.history, self.action_space_size)"
      ],
      "metadata": {
        "id": "rSCKGOM9zjwa"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, config: Config):\n",
        "    self.window_size = config.window_size\n",
        "    self.batch_size = config.batch_size\n",
        "    self.buffer = []\n",
        "\n",
        "  def save_game(self, game):\n",
        "    if len(self.buffer) > self.window_size:\n",
        "      self.buffer.pop(0)\n",
        "    self.buffer.append(game)\n",
        "\n",
        "  def sample_batch(self, num_unroll_steps: int, td_steps: int):\n",
        "    games = [self.sample_game() for _ in range(self.batch_size)]\n",
        "    game_pos = [(g, self.sample_position(g)) for g in games]\n",
        "    return [(g.make_image(i), g.history[i:i + num_unroll_steps],\n",
        "             g.make_target(i, num_unroll_steps, td_steps, g.to_play()))\n",
        "            for (g, i) in game_pos]\n",
        "\n",
        "  def sample_game(self) -> Game:\n",
        "    # Sample game from buffer either uniformly or according to some priority.\n",
        "    return self.buffer[0]\n",
        "\n",
        "  def sample_position(self, game) -> int:\n",
        "    # Sample position from game either uniformly or according to some priority.\n",
        "    return -1"
      ],
      "metadata": {
        "id": "En06LbK6zoRk"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetworkOutput(typing.NamedTuple):\n",
        "  value: float\n",
        "  reward: float\n",
        "  policy_logits: Dict[Action, float]\n",
        "  hidden_state: List[float]"
      ],
      "metadata": {
        "id": "Js8F5eaeEcp8"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(object):\n",
        "\n",
        "  def __init__(self, config: Config, uniform_network: bool=False):\n",
        "    self.steps = 0\n",
        "    if(uniform_network):\n",
        "      self.model=UniformChessNetwork(config)\n",
        "    else:\n",
        "      self.model=ChessNetwork(config)\n",
        "\n",
        "  def initial_inference(self, image) -> NetworkOutput:\n",
        "    # representation + prediction function\n",
        "\n",
        "    # Use the prediction function to obtain policy logits, value\n",
        "    return self.model(image)\n",
        "\n",
        "\n",
        "  def recurrent_inference(self, hidden_state, action) -> NetworkOutput:\n",
        "    # dynamics + prediction function\n",
        "    # Apply the dynamics function to get the next hidden state and reward\n",
        "    return self.model(hidden_state, action)\n",
        "\n",
        "\n",
        "  def get_weights(self):\n",
        "    # Returns the weights of this network.\n",
        "      return self.model.get_weights()\n",
        "\n",
        "  def training_steps(self) -> int:\n",
        "    # How many steps / batches the network has been trained for.\n",
        "    return self.steps"
      ],
      "metadata": {
        "id": "HPr_d9UOzqBG"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SharedStorage(object):\n",
        "\n",
        "  def __init__(self, config: Config):\n",
        "    self._networks = {}\n",
        "    self._config = config\n",
        "\n",
        "  def latest_network(self) -> Network:\n",
        "    if self._networks:\n",
        "      return self._networks[max(self._networks.keys())]\n",
        "    else:\n",
        "      # policy -> uniform, value -> 0, reward -> 0\n",
        "      return Network(config, True)\n",
        "\n",
        "  def save_network(self, step: int, network: Network):\n",
        "    self._networks[step] = network"
      ],
      "metadata": {
        "id": "6UsTnTQKzreQ"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split into two independent parts: Network training and\n",
        "# self-play data generation.\n",
        "# These two parts only communicate by transferring the latest network checkpoint\n",
        "# from the training to the self-play, and the finished games from the self-play\n",
        "# to the training.\n",
        "def zero(config: Config):\n",
        "  storage = SharedStorage(config)\n",
        "  replay_buffer = ReplayBuffer(config)\n",
        "\n",
        "  for _ in range(config.num_actors):\n",
        "    launch_job(run_selfplay, config, storage, replay_buffer)\n",
        "\n",
        "  train_network(config, storage, replay_buffer)\n",
        "\n",
        "  return storage.latest_network()"
      ],
      "metadata": {
        "id": "tV9QkBmLBKT9"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "####### Part 1: Self-Play ########\n",
        "\n",
        "\n",
        "# Each self-play job is independent of all others; it takes the latest network\n",
        "# snapshot, produces a game and makes it available to the training job by\n",
        "# writing it to a shared replay buffer.\n",
        "def run_selfplay(config: Config, storage: SharedStorage,\n",
        "                 replay_buffer: ReplayBuffer):\n",
        "  while True:\n",
        "    network = storage.latest_network()\n",
        "    game = play_game(config, network)\n",
        "    replay_buffer.save_game(game)\n",
        "\n",
        "\n",
        "# Each game is produced by starting at the initial board position, then\n",
        "# repeatedly executing a Monte Carlo Tree Search to generate moves until the end\n",
        "# of the game is reached.\n",
        "def play_game(config: Config, network: Network) -> Game:\n",
        "  game = config.new_game()\n",
        "\n",
        "  while not game.terminal() and len(game.history) < config.max_moves:\n",
        "    # At the root of the search tree we use the representation function to\n",
        "    # obtain a hidden state given the current observation.\n",
        "    root = Node(0)\n",
        "    current_observation = game.make_image(-1)\n",
        "    expand_node(root, game.to_play(), game.legal_actions(),\n",
        "                network.initial_inference(current_observation))\n",
        "    add_exploration_noise(config, root)\n",
        "\n",
        "    # We then run a Monte Carlo Tree Search using only action sequences and the\n",
        "    # model learned by the network.\n",
        "    run_mcts(config, root, game.action_history(), network)\n",
        "    action = select_action(config, len(game.history), root, network)\n",
        "    game.apply(action)\n",
        "    game.store_search_statistics(root)\n",
        "  return game\n",
        "\n",
        "\n",
        "# Core Monte Carlo Tree Search algorithm.\n",
        "# To decide on an action, we run N simulations, always starting at the root of\n",
        "# the search tree and traversing the tree according to the UCB formula until we\n",
        "# reach a leaf node.\n",
        "def run_mcts(config: Config, root: Node, action_history: ActionHistory,\n",
        "             network: Network):\n",
        "  min_max_stats = MinMaxStats(config.known_bounds)\n",
        "\n",
        "  for _ in range(config.num_simulations):\n",
        "    history = action_history.clone()\n",
        "    node = root\n",
        "    search_path = [node]\n",
        "\n",
        "    while node.expanded():\n",
        "      action, node = select_child(config, node, min_max_stats)\n",
        "      history.add_action(action)\n",
        "      search_path.append(node)\n",
        "\n",
        "    # Inside the search tree we use the dynamics function to obtain the next\n",
        "    # hidden state given an action and the previous hidden state.\n",
        "    parent = search_path[-2]\n",
        "    network_output = network.recurrent_inference(parent.hidden_state,\n",
        "                                                 history.last_action())\n",
        "    expand_node(node, history.to_play(), history.action_space(), network_output)\n",
        "\n",
        "    backpropagate(search_path, network_output.value, history.to_play(),\n",
        "                  config.discount, min_max_stats)\n",
        "\n",
        "\n",
        "\n",
        "def softmax_sample(visit_counts, temperature=10.0):\n",
        "    counts, actions = zip(*visit_counts)\n",
        "    # Apply softmax with temperature\n",
        "    counts = np.array(counts)\n",
        "    counts = counts / temperature  # Apply temperature scaling\n",
        "    softmax_probs = np.exp(counts) / sum(np.exp(counts))\n",
        "    # Sample an action based on the softmax probabilities\n",
        "    action = np.random.choice(actions, p=softmax_probs)\n",
        "    return softmax_probs, action\n",
        "\n",
        "\n",
        "def select_action(config: Config, num_moves: int, node: Node,\n",
        "                  network: Network):\n",
        "  visit_counts = [\n",
        "      (child.visit_count, action) for action, child in node.children.items()\n",
        "  ]\n",
        "  t = config.visit_softmax_temperature_fn(\n",
        "      num_moves=num_moves, training_steps=network.training_steps())\n",
        "  _, action = softmax_sample(visit_counts, t)\n",
        "  return action\n",
        "\n",
        "\n",
        "# Select the child with the highest UCB score.\n",
        "def select_child(config: Config, node: Node,\n",
        "                 min_max_stats: MinMaxStats):\n",
        "  _, action, child = max(\n",
        "      (ucb_score(config, node, child, min_max_stats), action,\n",
        "       child) for action, child in node.children.items())\n",
        "  return action, child\n",
        "\n",
        "\n",
        "# The score for a node is based on its value, plus an exploration bonus based on\n",
        "# the prior.\n",
        "def ucb_score(config: Config, parent: Node, child: Node,\n",
        "              min_max_stats: MinMaxStats) -> float:\n",
        "  pb_c = math.log((parent.visit_count + config.pb_c_base + 1) /\n",
        "                  config.pb_c_base) + config.pb_c_init\n",
        "  pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "  prior_score = pb_c * child.prior\n",
        "  value_score = min_max_stats.normalize(child.value())\n",
        "  return prior_score + value_score\n",
        "\n",
        "# We expand a node using the value, reward and policy prediction obtained from\n",
        "# the neural network.\n",
        "def expand_node(node: Node, to_play, actions: List[Action],\n",
        "                network_output: NetworkOutput):\n",
        "  node.to_play = to_play\n",
        "  node.hidden_state = network_output.hidden_state\n",
        "  node.reward = network_output.reward\n",
        "  policy = {a: math.exp(network_output.policy_logits[a]) for a in actions}\n",
        "  policy_sum = sum(policy.values())\n",
        "  for action, p in policy.items():\n",
        "    node.children[action] = Node(p / policy_sum)\n",
        "\n",
        "\n",
        "# At the end of a simulation, we propagate the evaluation all the way up the\n",
        "# tree to the root.\n",
        "def backpropagate(search_path: List[Node], value: float, to_play,\n",
        "                  discount: float, min_max_stats: MinMaxStats):\n",
        "  for node in search_path:\n",
        "    node.value_sum += value if node.to_play == to_play else (1-value)\n",
        "    node.visit_count += 1\n",
        "    min_max_stats.update(node.value())\n",
        "\n",
        "    value = node.reward + discount * value\n",
        "\n",
        "\n",
        "# At the start of each search, we add dirichlet noise to the prior of the root\n",
        "# to encourage the search to explore new actions.\n",
        "def add_exploration_noise(config: Config, node: Node):\n",
        "  actions = list(node.children.keys())\n",
        "  noise = numpy.random.dirichlet([config.root_dirichlet_alpha] * len(actions))\n",
        "  frac = config.root_exploration_fraction\n",
        "  for a, n in zip(actions, noise):\n",
        "    node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "######### End Self-Play ##########\n",
        "##################################\n",
        "\n",
        "##################################\n",
        "####### Part 2: Training #########\n",
        "\n",
        "\n",
        "def train_network(config: Config, storage: SharedStorage,\n",
        "                  replay_buffer: ReplayBuffer):\n",
        "  network = Network(config)\n",
        "  learning_rate = config.lr_init * config.lr_decay_rate**(\n",
        "      tf.train.get_global_step() / config.lr_decay_steps)\n",
        "  optimizer = tf.train.SGD(learning_rate, config.momentum)\n",
        "\n",
        "  for i in range(config.training_steps):\n",
        "    if i % config.checkpoint_interval == 0:\n",
        "      storage.save_network(i, network)\n",
        "    batch = replay_buffer.sample_batch(config.num_unroll_steps, config.td_steps)\n",
        "    update_weights(optimizer, network, batch, config.weight_decay)\n",
        "    network.steps += 1\n",
        "  storage.save_network(config.training_steps, network)\n",
        "\n",
        "def update_weights(optimizer: tf.keras.optimizers, network: Network, batch,\n",
        "                   weight_decay: float):\n",
        "  loss = 0\n",
        "  for image, actions, targets in batch:\n",
        "    # Initial step, from the real observation.\n",
        "    value, reward, policy_logits, hidden_state = network.initial_inference(\n",
        "        image)\n",
        "    predictions = [(1.0, value, reward, policy_logits)]\n",
        "\n",
        "    # Recurrent steps, from action and previous hidden state.\n",
        "    for action in actions:\n",
        "      value, reward, policy_logits, hidden_state = network.recurrent_inference(\n",
        "          hidden_state, action)\n",
        "      predictions.append((1.0 / len(actions), value, reward, policy_logits))\n",
        "\n",
        "      hidden_state = tf.scale_gradient(hidden_state, 0.5)\n",
        "\n",
        "    for prediction, target in zip(predictions, targets):\n",
        "      gradient_scale, value, reward, policy_logits = prediction\n",
        "      target_value, target_reward, target_policy = target\n",
        "\n",
        "      l = (\n",
        "          scalar_loss(value, target_value) +\n",
        "          scalar_loss(reward, target_reward) +\n",
        "          tf.nn.softmax_cross_entropy_with_logits(\n",
        "              logits=policy_logits, labels=target_policy))\n",
        "\n",
        "      loss += tf.scale_gradient(l, gradient_scale)\n",
        "\n",
        "  for weights in network.get_weights():\n",
        "    loss += weight_decay * tf.nn.l2_loss(weights)\n",
        "\n",
        "  optimizer.minimize(loss)\n",
        "\n",
        "\n",
        "######### End Training ###########\n",
        "##################################\n",
        "\n",
        "def scalar_loss(prediction, target) -> float:\n",
        "    squared_error = (prediction - target) ** 2\n",
        "    mse = squared_error.mean()\n",
        "    return mse\n",
        "\n",
        "def launch_job(f, *args):\n",
        "  f(*args)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ou5wkaIzpvvk"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = make_chess_config()\n",
        "network_1 = zero(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "tsgLx3x98qnB",
        "outputId": "b87ac21b-708a-497c-f16d-41f3751e3afb"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling layer 'uniform_chess_network_6' (type UniformChessNetwork).\n\nLayer \"conv2d_396\" expects 1 input(s), but it received 0 input tensors. Inputs received: []\n\nCall arguments received by layer 'uniform_chess_network_6' (type UniformChessNetwork):\n  • inputs=[]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-21e14a057da4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_chess_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnetwork_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-146-cd70253fc97a>\u001b[0m in \u001b[0;36mzero\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mlaunch_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_selfplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-147-35968db04c02>\u001b[0m in \u001b[0;36mlaunch_job\u001b[0;34m(f, *args)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlaunch_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m   \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-147-35968db04c02>\u001b[0m in \u001b[0;36mrun_selfplay\u001b[0;34m(config, storage, replay_buffer)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-147-35968db04c02>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(config, network)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mcurrent_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     expand_node(root, game.to_play(), game.legal_actions(),\n\u001b[0;32m---> 28\u001b[0;31m                 network.initial_inference(current_observation))\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0madd_exploration_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-144-9f2f7e8f1e49>\u001b[0m in \u001b[0;36minitial_inference\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Use the prediction function to obtain policy logits, value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-134-90d392ea94d4>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Implement call method to process inputs through representation and dynamics functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Process inputs through the representation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation_res_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'uniform_chess_network_6' (type UniformChessNetwork).\n\nLayer \"conv2d_396\" expects 1 input(s), but it received 0 input tensors. Inputs received: []\n\nCall arguments received by layer 'uniform_chess_network_6' (type UniformChessNetwork):\n  • inputs=[]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LjvDlKC-83UJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}