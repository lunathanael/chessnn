{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVB1+Pq61gr4q6mmCjX0W9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lunathanael/chessnn/blob/main/policy_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEOVZHf_fRtr"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "from typing import List\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Add, Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_network():\n",
        "  def residual_block(x, filters):\n",
        "      \"\"\"Create a residual block.\"\"\"\n",
        "      y = Conv2D(filters, kernel_size=3, padding='same')(x)\n",
        "      y = BatchNormalization()(y)\n",
        "      y = ReLU()(y)\n",
        "      y = Conv2D(filters, kernel_size=3, padding='same')(y)\n",
        "      y = BatchNormalization()(y)\n",
        "      y = Add()([y, x])\n",
        "      y = ReLU()(y)\n",
        "      return y\n",
        "\n",
        "  # Input layer\n",
        "  input_layer = Input(shape=(8, 8, 73))\n",
        "\n",
        "  # Body\n",
        "  x = Conv2D(256, kernel_size=3, padding='same')(input_layer)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "  for _ in range(19):\n",
        "      x = residual_block(x, 256)\n",
        "\n",
        "  # Policy Head\n",
        "  policy_head = Conv2D(256, kernel_size=3, padding='same')(x)\n",
        "  policy_head = BatchNormalization()(policy_head)\n",
        "  policy_head = ReLU()(policy_head)\n",
        "  policy_head = Conv2D(73, kernel_size=1)(policy_head)\n",
        "\n",
        "  # Value Head\n",
        "  value_head = Conv2D(1, kernel_size=1)(x)\n",
        "  value_head = BatchNormalization()(value_head)\n",
        "  value_head = ReLU()(value_head)\n",
        "  value_head = Dense(256, activation='relu')(value_head)\n",
        "  value_head = Dense(1, activation='tanh')(value_head)\n",
        "\n",
        "  # Create the model\n",
        "  model = tf.keras.Model(inputs=input_layer, outputs=[policy_head, value_head])\n",
        "  return model\n",
        "\n",
        "def make_uniform_network():\n",
        "\n",
        "    # Modified Policy Head for uniform output\n",
        "    policy_head = Lambda(lambda x: K.constant(1/73, shape=(8, 8, 73)))(x)\n",
        "\n",
        "    # Modified Value Head for constant output\n",
        "    value_head = Lambda(lambda x: K.constant(0.5))(x)\n",
        "\n",
        "    # Create the model with constant outputs\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=[policy_head, value_head])\n",
        "    return model"
      ],
      "metadata": {
        "id": "ABgGpmx0fWGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlphaZeroConfig(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    ### Self-Play\n",
        "    self.num_actors = 5000\n",
        "\n",
        "    self.num_sampling_moves = 30\n",
        "    self.max_moves = 512  # for chess and shogi, 722 for Go.\n",
        "    self.num_simulations = 800\n",
        "\n",
        "    # Root prior exploration noise.\n",
        "    self.root_dirichlet_alpha = 0.3  # for chess, 0.03 for Go and 0.15 for shogi.\n",
        "    self.root_exploration_fraction = 0.25\n",
        "\n",
        "    # UCB formula\n",
        "    self.pb_c_base = 19652\n",
        "    self.pb_c_init = 1.25\n",
        "\n",
        "    ### Training\n",
        "    self.training_steps = int(700e3)\n",
        "    self.checkpoint_interval = int(1e3)\n",
        "    self.window_size = int(1e6)\n",
        "    self.batch_size = 4096\n",
        "\n",
        "    self.weight_decay = 1e-4\n",
        "    self.momentum = 0.9\n",
        "    # Schedule for chess and shogi, Go starts at 2e-2 immediately.\n",
        "    self.learning_rate_schedule = {\n",
        "        0: 2e-1,\n",
        "        100e3: 2e-2,\n",
        "        300e3: 2e-3,\n",
        "        500e3: 2e-4\n",
        "    }\n",
        "\n",
        "\n",
        "class Node(object):\n",
        "\n",
        "  def __init__(self, prior: float):\n",
        "    self.visit_count = 0\n",
        "    self.to_play = -1\n",
        "    self.prior = prior\n",
        "    self.value_sum = 0\n",
        "    self.children = {}\n",
        "\n",
        "  def expanded(self):\n",
        "    return len(self.children) > 0\n",
        "\n",
        "  def value(self):\n",
        "    if self.visit_count == 0:\n",
        "      return 0\n",
        "    return self.value_sum / self.visit_count\n",
        "\n",
        "\n",
        "class Game(object):\n",
        "\n",
        "  def __init__(self, history=None):\n",
        "    self.history = history or []\n",
        "    self.child_visits = []\n",
        "    self.num_actions = 4672  # action space size for chess; 11259 for shogi, 362 for Go\n",
        "\n",
        "  def terminal(self):\n",
        "    # Game specific termination rules.\n",
        "    pass\n",
        "\n",
        "  def terminal_value(self, to_play):\n",
        "    # Game specific value.\n",
        "    pass\n",
        "\n",
        "  def legal_actions(self):\n",
        "    # Game specific calculation of legal actions.\n",
        "    return []\n",
        "\n",
        "  def clone(self):\n",
        "    return Game(list(self.history))\n",
        "\n",
        "  def apply(self, action):\n",
        "    self.history.append(action)\n",
        "\n",
        "  def store_search_statistics(self, root):\n",
        "    sum_visits = sum(child.visit_count for child in root.children.itervalues())\n",
        "    self.child_visits.append([\n",
        "        root.children[a].visit_count / sum_visits if a in root.children else 0\n",
        "        for a in range(self.num_actions)\n",
        "    ])\n",
        "\n",
        "  def make_image(self, state_index: int):\n",
        "    # Game specific feature planes.\n",
        "    return []\n",
        "\n",
        "  def make_target(self, state_index: int):\n",
        "    return (self.terminal_value(state_index % 2),\n",
        "            self.child_visits[state_index])\n",
        "\n",
        "  def to_play(self):\n",
        "    return len(self.history) % 2\n",
        "\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, config: AlphaZeroConfig):\n",
        "    self.window_size = config.window_size\n",
        "    self.batch_size = config.batch_size\n",
        "    self.buffer = []\n",
        "\n",
        "  def save_game(self, game):\n",
        "    if len(self.buffer) > self.window_size:\n",
        "      self.buffer.pop(0)\n",
        "    self.buffer.append(game)\n",
        "\n",
        "  def sample_batch(self):\n",
        "    # Sample uniformly across positions.\n",
        "    move_sum = float(sum(len(g.history) for g in self.buffer))\n",
        "    games = numpy.random.choice(\n",
        "        self.buffer,\n",
        "        size=self.batch_size,\n",
        "        p=[len(g.history) / move_sum for g in self.buffer])\n",
        "    game_pos = [(g, numpy.random.randint(len(g.history))) for g in games]\n",
        "    return [(g.make_image(i), g.make_target(i)) for (g, i) in game_pos]\n",
        "\n",
        "\n",
        "class Network(object):\n",
        "\n",
        "  def __init__(self, uniform_model: bool=False):\n",
        "    if (uniform_model):\n",
        "      self.model = make_uniform_network()\n",
        "    else:\n",
        "      self.model= make_network()\n",
        "\n",
        "  def inference(self, image):\n",
        "    return (-1, {})  # Value, Policy\n",
        "\n",
        "  def get_weights(self):\n",
        "    # Returns the weights of this network.\n",
        "    return []\n",
        "\n",
        "\n",
        "class SharedStorage(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._networks = {}\n",
        "\n",
        "  def latest_network(self) -> Network:\n",
        "    if self._networks:\n",
        "      return self._networks[max(self._networks.iterkeys())]\n",
        "    else:\n",
        "      return Network(true)  # policy -> uniform, value -> 0.5\n",
        "\n",
        "  def save_network(self, step: int, network: Network):\n",
        "    self._networks[step] = network\n",
        "\n",
        "\n",
        "##### End Helpers ########\n",
        "##########################\n",
        "\n",
        "\n",
        "# AlphaZero training is split into two independent parts: Network training and\n",
        "# self-play data generation.\n",
        "# These two parts only communicate by transferring the latest network checkpoint\n",
        "# from the training to the self-play, and the finished games from the self-play\n",
        "# to the training.\n",
        "def alphazero(config: AlphaZeroConfig):\n",
        "  storage = SharedStorage()\n",
        "  replay_buffer = ReplayBuffer(config)\n",
        "\n",
        "  for i in range(config.num_actors):\n",
        "    launch_job(run_selfplay, config, storage, replay_buffer)\n",
        "\n",
        "  train_network(config, storage, replay_buffer)\n",
        "\n",
        "  return storage.latest_network()\n",
        "\n",
        "\n",
        "##################################\n",
        "####### Part 1: Self-Play ########\n",
        "\n",
        "\n",
        "# Each self-play job is independent of all others; it takes the latest network\n",
        "# snapshot, produces a game and makes it available to the training job by\n",
        "# writing it to a shared replay buffer.\n",
        "def run_selfplay(config: AlphaZeroConfig, storage: SharedStorage,\n",
        "                 replay_buffer: ReplayBuffer):\n",
        "  while True:\n",
        "    network = storage.latest_network()\n",
        "    game = play_game(config, network)\n",
        "    replay_buffer.save_game(game)\n",
        "\n",
        "\n",
        "# Each game is produced by starting at the initial board position, then\n",
        "# repeatedly executing a Monte Carlo Tree Search to generate moves until the end\n",
        "# of the game is reached.\n",
        "def play_game(config: AlphaZeroConfig, network: Network):\n",
        "  game = Game()\n",
        "  while not game.terminal() and len(game.history) < config.max_moves:\n",
        "    action, root = run_mcts(config, game, network)\n",
        "    game.apply(action)\n",
        "    game.store_search_statistics(root)\n",
        "  return game\n",
        "\n",
        "\n",
        "# Core Monte Carlo Tree Search algorithm.\n",
        "# To decide on an action, we run N simulations, always starting at the root of\n",
        "# the search tree and traversing the tree according to the UCB formula until we\n",
        "# reach a leaf node.\n",
        "def run_mcts(config: AlphaZeroConfig, game: Game, network: Network):\n",
        "  root = Node(0)\n",
        "  evaluate(root, game, network)\n",
        "  add_exploration_noise(config, root)\n",
        "\n",
        "  for _ in range(config.num_simulations):\n",
        "    node = root\n",
        "    scratch_game = game.clone()\n",
        "    search_path = [node]\n",
        "\n",
        "    while node.expanded():\n",
        "      action, node = select_child(config, node)\n",
        "      scratch_game.apply(action)\n",
        "      search_path.append(node)\n",
        "\n",
        "    value = evaluate(node, scratch_game, network)\n",
        "    backpropagate(search_path, value, scratch_game.to_play())\n",
        "  return select_action(config, game, root), root\n",
        "\n",
        "\n",
        "def select_action(config: AlphaZeroConfig, game: Game, root: Node):\n",
        "  visit_counts = [(child.visit_count, action)\n",
        "                  for action, child in root.children.iteritems()]\n",
        "  if len(game.history) < config.num_sampling_moves:\n",
        "    _, action = softmax_sample(visit_counts)\n",
        "  else:\n",
        "    _, action = max(visit_counts)\n",
        "  return action\n",
        "\n",
        "\n",
        "# Select the child with the highest UCB score.\n",
        "def select_child(config: AlphaZeroConfig, node: Node):\n",
        "  _, action, child = max((ucb_score(config, node, child), action, child)\n",
        "                         for action, child in node.children.iteritems())\n",
        "  return action, child\n",
        "\n",
        "\n",
        "# The score for a node is based on its value, plus an exploration bonus based on\n",
        "# the prior.\n",
        "def ucb_score(config: AlphaZeroConfig, parent: Node, child: Node):\n",
        "  pb_c = math.log((parent.visit_count + config.pb_c_base + 1) /\n",
        "                  config.pb_c_base) + config.pb_c_init\n",
        "  pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "  prior_score = pb_c * child.prior\n",
        "  value_score = child.value()\n",
        "  return prior_score + value_score\n",
        "\n",
        "\n",
        "# We use the neural network to obtain a value and policy prediction.\n",
        "def evaluate(node: Node, game: Game, network: Network):\n",
        "  value, policy_logits = network.inference(game.make_image(-1))\n",
        "\n",
        "  # Expand the node.\n",
        "  node.to_play = game.to_play()\n",
        "  policy = {a: math.exp(policy_logits[a]) for a in game.legal_actions()}\n",
        "  policy_sum = sum(policy.itervalues())\n",
        "  for action, p in policy.iteritems():\n",
        "    node.children[action] = Node(p / policy_sum)\n",
        "  return value\n",
        "\n",
        "\n",
        "# At the end of a simulation, we propagate the evaluation all the way up the\n",
        "# tree to the root.\n",
        "def backpropagate(search_path: List[Node], value: float, to_play):\n",
        "  for node in search_path:\n",
        "    node.value_sum += value if node.to_play == to_play else (1 - value)\n",
        "    node.visit_count += 1\n",
        "\n",
        "\n",
        "# At the start of each search, we add dirichlet noise to the prior of the root\n",
        "# to encourage the search to explore new actions.\n",
        "def add_exploration_noise(config: AlphaZeroConfig, node: Node):\n",
        "  actions = node.children.keys()\n",
        "  noise = numpy.random.gamma(config.root_dirichlet_alpha, 1, len(actions))\n",
        "  frac = config.root_exploration_fraction\n",
        "  for a, n in zip(actions, noise):\n",
        "    node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "######### End Self-Play ##########\n",
        "##################################\n",
        "\n",
        "##################################\n",
        "####### Part 2: Training #########\n",
        "\n",
        "\n",
        "def train_network(config: AlphaZeroConfig, storage: SharedStorage,\n",
        "                  replay_buffer: ReplayBuffer):\n",
        "  network = Network()\n",
        "  optimizer = tf.train.MomentumOptimizer(config.learning_rate_schedule,\n",
        "                                         config.momentum)\n",
        "  for i in range(config.training_steps):\n",
        "    if i % config.checkpoint_interval == 0:\n",
        "      storage.save_network(i, network)\n",
        "    batch = replay_buffer.sample_batch()\n",
        "    update_weights(optimizer, network, batch, config.weight_decay)\n",
        "  storage.save_network(config.training_steps, network)\n",
        "\n",
        "def update_weights(optimizer: tf.compat.v1.train.Optimizer, network: Network, batch,\n",
        "                   weight_decay: float):\n",
        "  loss = 0\n",
        "  for image, (target_value, target_policy) in batch:\n",
        "    value, policy_logits = network.inference(image)\n",
        "    loss += (\n",
        "        tf.losses.mean_squared_error(value, target_value) +\n",
        "        tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=policy_logits, labels=target_policy))\n",
        "\n",
        "  for weights in network.get_weights():\n",
        "    loss += weight_decay * tf.nn.l2_loss(weights)\n",
        "\n",
        "  optimizer.minimize(loss)\n",
        "\n",
        "\n",
        "######### End Training ###########\n",
        "##################################\n",
        "\n",
        "################################################################################\n",
        "############################# End of pseudocode ################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "# Stubs to make the typechecker happy, should not be included in pseudocode\n",
        "# for the paper.\n",
        "def softmax_sample(d):\n",
        "  return 0, 0\n",
        "\n",
        "\n",
        "def launch_job(f, *args):\n",
        "  f(*args)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ou5wkaIzpvvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "IpFpCykbfahl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def fen_to_board_array(fen):\n",
        "    # Split the FEN string to get the relevant parts\n",
        "    parts = fen.split(' ')\n",
        "    board_fen, player, castling, _, halfmove, fullmove = parts[:6]\n",
        "\n",
        "    # Initialize an empty board with additional planes for L constant-valued inputs\n",
        "    board = np.zeros((12 + 5, 8, 8), dtype=float)  # 12 for pieces, 5 for additional planes\n",
        "\n",
        "    # Define piece order and mapping to layers\n",
        "    piece_map = {'p': 0, 'n': 1, 'b': 2, 'r': 3, 'q': 4, 'k': 5,\n",
        "                 'P': 6, 'N': 7, 'B': 8, 'R': 9, 'Q': 10, 'K': 11}\n",
        "\n",
        "    # Fill the board with pieces\n",
        "    row = 0\n",
        "    col = 0\n",
        "    for char in board_fen:\n",
        "        if char.isdigit():\n",
        "            col += int(char)\n",
        "        elif char == '/':\n",
        "            row += 1\n",
        "            col = 0\n",
        "        else:\n",
        "            board[piece_map[char], row, col] = 1\n",
        "            col += 1\n",
        "\n",
        "    # Add extra layers for player color, move number, etc.\n",
        "    # Player color (1 for black, 0 for white)\n",
        "    board[12, :, :] = 1 if player == 'b' else 0\n",
        "\n",
        "    # Castling rights encoded in four binary planes\n",
        "    board[13, :, :] = 1 if 'K' in castling else 0  # White kingside\n",
        "    board[14, :, :] = 1 if 'Q' in castling else 0  # White queenside\n",
        "    board[15, :, :] = 1 if 'k' in castling else 0  # Black kingside\n",
        "    board[16, :, :] = 1 if 'q' in castling else 0  # Black queenside\n",
        "\n",
        "    # Move number (as a real value)\n",
        "    board[17, :, :] = float(fullmove)\n",
        "\n",
        "    return board\n",
        "\n",
        "# Example usage\n",
        "fen = \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n",
        "board_array = fen_to_board_array(fen)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Kb78XOxjZek",
        "outputId": "cfc81f8c-1ea5-469b-cfd0-aa0d36832d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "\n",
              "       [[0, 1, 0, 0, 0, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 1, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "\n",
              "       [[1, 0, 0, 0, 0, 0, 0, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, 1, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, 0, 1, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 1, 0, 0, 0, 0, 1, 0]],\n",
              "\n",
              "       [[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 1, 0, 0, 1, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 0, 0, 0, 1]],\n",
              "\n",
              "       [[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 1, 0, 0, 0]]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}